---
lab:
  title: Analyser des donn√©es avec Apache Spark
  module: Use Apache Spark to work with files in a lakehouse
---

# Analyser des donn√©es avec Apache Spark dans Fabric

Dans ce labo, vous allez ing√©rer des donn√©es dans le lakehouse Fabric et utiliser PySpark pour lire et analyser les donn√©es.

Ce labo prend environ 45¬†minutes.

## Pr√©requis

* [Version d‚Äô√©valuation de Microsoft¬†Fabric](/fabric/get-started/fabric-trial#start-the-fabric-capacity-trial)

## Cr√©er un espace de travail

Avant de pouvoir utiliser des donn√©es dans Fabric, vous devez cr√©er un espace de travail.

1. Sur la page d‚Äôaccueil de [Microsoft Fabric](https://app.fabric.microsoft.com) √† l‚Äôadresse https://app.fabric.microsoft.com, s√©lectionnez l‚Äôexp√©rience **Engineering donn√©es**.
1. Dans la barre de navigation de gauche, s√©lectionnez **Espaces de travail ** (üóá), puis **Nouvel espace de travail**.
1. Nommez le nouvel espace de travail et, dans la section **Avanc√©**, s√©lectionnez le mode de licence appropri√©. Si vous avez d√©marr√© une version d‚Äô√©valuation de Microsoft¬†Fabric, s√©lectionnez Version d‚Äô√©valuation.
1. S√©lectionnez **Appliquer** pour cr√©er et ouvrir l‚Äôespace de travail.
 
![Image d‚Äô√©cran des fichiers CSV charg√©s dans un nouvel espace de travail Fabric.](Images/uploaded-files.jpg)

## Cr√©er un lakehouse et charger des fichiers

Maintenant que vous disposez d‚Äôun espace de travail, vous pouvez cr√©er un lakehouse pour vos fichiers de donn√©es. Dans votre nouvel espace de travail, s√©lectionnez **Nouveau** et **Lakehouse**. Nommez le lakehouse, puis s√©lectionnez **Cr√©er**. Apr√®s un court d√©lai, un nouveau lakehouse est cr√©√©.

Vous pouvez d√©sormais ing√©rer des donn√©es dans le lakehouse. Il existe plusieurs fa√ßons de proc√©der, mais vous allez pour le moment t√©l√©charger un dossier de fichiers texte de votre ordinateur local (ou machine virtuelle de labo le cas √©ch√©ant), puis les charger dans votre lakehouse.

1. T√©l√©chargez tous les fichiers de donn√©es √† partir de https://github.com/MicrosoftLearning/dp-data/raw/main/orders.zip.
1. Extrayez l‚Äôarchive compress√©e et v√©rifiez que vous disposez d‚Äôun dossier nomm√© *orders* qui contient trois fichiers¬†CSV¬†: 2019.csv, 2020.csv et 2021.csv.
1. Revenez √† votre nouveau lakehouse. Dans le volet **Explorateur**, en regard du dossier **Fichiers**, s√©lectionnez le menu **‚Ä¶**, puis s√©lectionnez **Charger** et **Charger le dossier**. Acc√©dez au dossier orders sur votre ordinateur local (ou machine virtuelle de labo le cas √©ch√©ant) et s√©lectionnez **Charger**.
1. Une fois les fichiers charg√©s, d√©veloppez **Files** et s√©lectionnez le dossier **orders**. V√©rifiez que les fichiers CSV ont √©t√© charg√©s, comme indiqu√© ici¬†:

![Image d‚Äô√©cran d‚Äôun nouvel espace de travail Fabric.](Images/new-workspace.jpg)

## Cr√©er un notebook

Vous pouvez maintenant cr√©er un notebook Fabric pour utiliser vos donn√©es. Les notebooks fournissent un environnement interactif dans lequel vous pouvez √©crire et ex√©cuter du code (dans plusieurs langues).

1. S√©lectionnez votre espace de travail, puis s√©lectionnez **Nouveau** et **Notebook**. Apr√®s quelques secondes, un nouveau notebook contenant une seule cellule s‚Äôouvre. Les notebooks sont constitu√©s d‚Äôune ou plusieurs cellules qui peuvent contenir du code ou du Markdown (texte mis en forme).
1. Fabric attribue un nom √† chaque notebook que vous cr√©ez, tel que Notebook¬†1, Notebook¬†2, etc. Cliquez sur le panneau de noms au-dessus de l‚Äôonglet **Accueil** du menu pour remplacer le nom par quelque chose de plus descriptif.
1. S√©lectionnez la premi√®re cellule (qui est actuellement une cellule de code) puis, dans la barre d‚Äôoutils en haut √† droite, utilisez le bouton **M‚Üì** pour convertir la cellule en cellule Markdown. Le texte contenu dans la cellule s‚Äôaffiche alors sous forme de texte mis en forme.
1. Utilisez le bouton üñâ (Modifier) pour placer la cellule en mode √©dition, puis modifiez le balisage Markdown comme suit.

```markdown
# Sales order data exploration
Use this notebook to explore sales order data
```
![Image d‚Äô√©cran d‚Äôun notebook Fabric avec une cellule Markdown.](Images/name-notebook-markdown.jpg)

Lorsque vous avez termin√©, cliquez n‚Äôimporte o√π dans le notebook en dehors de la cellule pour arr√™ter sa modification et voir le balisage Markdown rendu.


## Cr√©er un DataFrame

Maintenant que vous avez cr√©√© un espace de travail, un lakehouse et un notebook, vous pouvez utiliser vos donn√©es. Vous allez utiliser PySpark, qui est le langage par d√©faut pour les notebooks Fabric et la version de Python optimis√©e pour Spark.

**REMARQUE¬†:** les notebooks Fabric prennent en charge plusieurs langages de programmation, notamment Scala, R et Spark SQL.

1. S√©lectionnez votre nouvel espace de travail dans la barre de gauche. Vous verrez une liste d‚Äô√©l√©ments contenus dans l‚Äôespace de travail, y compris votre lakehouse et votre notebook.
2. S√©lectionnez le lakehouse pour afficher le volet Explorateur, y compris le dossier **orders**.
3. Dans le menu sup√©rieur, s√©lectionnez **Ouvrir le notebook**, **Notebook existant**, puis ouvrez le notebook que vous avez cr√©√© pr√©c√©demment. Le notebook doit maintenant √™tre ouvert en regard du volet Explorateur. D√©veloppez Lakehouses, d√©veloppez la liste Files et s√©lectionnez le dossier orders. Les fichiers CSV que vous avez charg√©s sont r√©pertori√©s en regard de l‚Äô√©diteur de notebook, comme suit¬†:

![Image d‚Äô√©cran des fichiers¬†CSV en mode Explorateur.](Images/explorer-notebook-view.jpg)

4. Dans le menu ‚Ä¶ pour 2019.csv, s√©lectionnez **Charger des donn√©es** > **Spark**. Le code suivant est automatiquement g√©n√©r√© dans une nouvelle cellule de code¬†:

```python
df = spark.read.format("csv").option("header","true").load("Files/orders/2019.csv")
# df now is a Spark DataFrame containing CSV data from "Files/orders/2019.csv".
display(df)
```

**Conseil¬†:** vous pouvez masquer les volets de l‚Äôexplorateur Lakehouse √† gauche √† l‚Äôaide des ic√¥nes ¬´. Cela donne plus d‚Äôespace pour le notebook.

5. S√©lectionnez ‚ñ∑ **Ex√©cuter la cellule** √† gauche de la cellule pour ex√©cuter le code.

**REMARQUE¬†:** la premi√®re fois que vous ex√©cutez du code Spark, une session Spark est d√©marr√©e. Cette op√©ration peut prendre quelques secondes, voire plus. Les ex√©cutions suivantes dans la m√™me session seront plus rapides.

6. Une fois le code de la cellule ex√©cut√©, examinez la sortie sous la cellule, qui doit ressembler √† ceci¬†:
 
![Image d‚Äô√©cran montrant le code et les donn√©es g√©n√©r√©s automatiquement.](Images/auto-generated-load.jpg)

7. La sortie affiche les donn√©es du fichier 2019.csv affich√©es en lignes et en colonnes.  Notez que les en-t√™tes de colonne contiennent la premi√®re ligne des donn√©es. Pour corriger ce probl√®me, vous devez modifier la premi√®re ligne du code comme suit¬†:

```python
df = spark.read.format("csv").option("header","false").load("Files/orders/2019.csv")
```

8. R√©ex√©cutez le code afin que le DataFrame identifie correctement la premi√®re ligne en tant que donn√©es. Notez que les noms de colonnes sont d√©sormais _c0, _c1, etc.

9. Les noms de colonnes descriptifs vous aident √† comprendre les donn√©es. Pour cr√©er des noms de colonnes explicites, vous devez d√©finir le sch√©ma et les types de donn√©es. Vous devez √©galement importer un ensemble standard de types SPARK SQL pour d√©finir les types de donn√©es. Remplacez le code existant par le code ci-dessous¬†:

```python
from pyspark.sql.types import *

orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
    ])

df = spark.read.format("csv").schema(orderSchema).load("Files/orders/2019.csv")

display(df)

```
10. Ex√©cutez la cellule et passez en revue la sortie¬†:

![Image d‚Äô√©cran du code avec sch√©ma d√©fini et donn√©es.](Images/define-schema.jpg)

11. Le DataFrame inclut uniquement les donn√©es du fichier 2019.csv. Modifiez le code afin que le chemin de fichier utilise un caract√®re g√©n√©rique * pour lire toutes les donn√©es dans le dossier orders¬†:

```python
from pyspark.sql.types import *

orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
    ])

df = spark.read.format("csv").schema(orderSchema).load("Files/orders/*.csv")

display(df)
```

12. Lorsque vous ex√©cutez le code modifi√©, vous devez voir les ventes pour 2019, 2020 et 2021. Seul un sous-ensemble des lignes est affich√©. Vous ne pouvez donc pas voir les lignes pour chaque ann√©e.

**REMARQUE¬†:** vous pouvez masquer ou afficher la sortie d‚Äôune cellule en s√©lectionnant ‚Ä¶ √† c√¥t√© du r√©sultat. Cela facilite l‚Äôutilisation d‚Äôun notebook.

## Explorer les donn√©es dans un DataFrame

L‚Äôobjet DataFrame fournit des fonctionnalit√©s suppl√©mentaires, telles que la possibilit√© de filtrer, de regrouper et de manipuler des donn√©es.

### Filtrer un DataFrame

1. Ajoutez une cellule de code en s√©lectionnant **+Code** qui appara√Æt lorsque vous pointez la souris au-dessus ou en dessous de la cellule active ou de sa sortie. Sinon, dans le menu du ruban, s√©lectionnez **Modifier** et **+Ajouter une cellule de code**.

2.  Le code suivant filtre les donn√©es afin que seules deux colonnes soient retourn√©es. Il utilise √©galement *count* et *distinct* pour r√©sumer le nombre d‚Äôenregistrements¬†:

```python
customers = df['CustomerName', 'Email']

print(customers.count())
print(customers.distinct().count())

display(customers.distinct())
```

3. Ex√©cutez le code et examinez la sortie.

* Le code cr√©e un DataFrame appel√© **customers** qui contient un sous-ensemble de colonnes √† partir du DataFrame **df** d‚Äôorigine. Lors de l‚Äôex√©cution d‚Äôune transformation DataFrame, vous ne modifiez pas le DataFrame d‚Äôorigine, mais vous en retournez un nouveau.
* Une autre fa√ßon d‚Äôobtenir le m√™me r√©sultat consiste √† utiliser la m√©thode select¬†:

```
customers = df.select("CustomerName", "Email")
```

* Les fonctions DataFrame * count* et *distinct* sont utilis√©es pour fournir des totaux pour le nombre de clients et les clients uniques.

4. Modifiez la premi√®re ligne du code √† l‚Äôaide de *select* avec une fonction *where* comme suit¬†:

```python
customers = df.select("CustomerName", "Email").where(df['Item']=='Road-250 Red, 52')
print(customers.count())
print(customers.distinct().count())

display(customers.distinct())
```

5. Ex√©cutez ce code modifi√© pour s√©lectionner uniquement les clients qui ont achet√© le produit Road-250 Red, 52. Notez que vous pouvez ¬´¬†attacher¬†¬ª plusieurs fonctions ensemble afin que la sortie d‚Äôune fonction devienne l‚Äôentr√©e de la suivante. Dans ce cas, le DataFrame cr√©√© par la m√©thode *select* est le DataFrame source pour la m√©thode **where** utilis√©e pour appliquer des crit√®res de filtrage.

### Agr√©ger et regrouper des donn√©es dans un DataFrame

1. Ajoutez une cellule de code et entrez le code suivant¬†:

```python
productSales = df.select("Item", "Quantity").groupBy("Item").sum()

display(productSales)
```

2. Ex√©cutez le code. Vous pouvez voir que les r√©sultats affichent la somme des quantit√©s de commandes regroup√©es par produit. La m√©thode *groupBy* regroupe les lignes par Item, et la fonction d‚Äôagr√©gation *sum* suivante est appliqu√©e √† toutes les colonnes num√©riques restantes, dans ce cas, *Quantity*.

3. Ajoutez une autre cellule de code au notebook, puis entrez le code suivant¬†:

```python
from pyspark.sql.functions import *

yearlySales = df.select(year(col("OrderDate")).alias("Year")).groupBy("Year").count().orderBy("Year")

display(yearlySales)
```

4. Ex√©cutez la cellule. Examinez la sortie. Cette fois, les r√©sultats indiquent le nombre de commandes client par an.

* L‚Äôinstruction *import* vous permet d‚Äôutiliser la biblioth√®que SPARK SQL.
* La m√©thode *select* est utilis√©e avec une fonction year SQL pour extraire le composant year du champ *OrderDate*.
* La m√©thode *alias* est utilis√©e pour affecter un nom de colonne √† la valeur d‚Äôann√©e extraite.
* La m√©thode *groupBy* regroupe les donn√©es par la colonne Year d√©riv√©e.
* Le nombre de lignes dans chaque groupe est calcul√© avant que la m√©thode *orderBy* soit utilis√©e pour trier le DataFrame r√©sultant.

![Image d‚Äô√©cran montrant les r√©sultats de l‚Äôagr√©gation et du regroupement de donn√©es dans un DataFrame.](Images/spark-sql-dataframe.jpg)

## Utiliser Spark pour transformer des fichiers de donn√©es

Une t√¢che courante des ing√©nieurs et scientifiques des donn√©es consiste √† transformer des donn√©es pour poursuivre leur traitement ou analyse en aval.

### Utiliser des m√©thodes et des fonctions de DataFrame pour transformer les donn√©es

1. Ajoutez une cellule de code au notebook, puis entrez ce qui suit¬†:

```python
from pyspark.sql.functions import *

# Create Year and Month columns
transformed_df = df.withColumn("Year", year(col("OrderDate"))).withColumn("Month", month(col("OrderDate")))

# Create the new FirstName and LastName fields
transformed_df = transformed_df.withColumn("FirstName", split(col("CustomerName"), " ").getItem(0)).withColumn("LastName", split(col("CustomerName"), " ").getItem(1))

# Filter and reorder columns
transformed_df = transformed_df["SalesOrderNumber", "SalesOrderLineNumber", "OrderDate", "Year", "Month", "FirstName", "LastName", "Email", "Item", "Quantity", "UnitPrice", "Tax"]

# Display the first five orders
display(transformed_df.limit(5))
```

2. Ex√©cutez la cellule. Un nouveau DataFrame est cr√©√© √† partir des donn√©es de commandes d‚Äôorigine avec les transformations suivantes¬†:

- Colonnes Year et Month ajout√©es, bas√©es sur la colonne OrderDate.
- Colonnes FirstName et LastName ajout√©es, bas√©es sur la colonne CustomerName.
- Les colonnes sont filtr√©es et r√©organis√©es, et la colonne CustomerName est supprim√©e.

3. Passez en revue la sortie et v√©rifiez que les transformations ont √©t√© apport√©es aux donn√©es.

Vous pouvez utiliser la biblioth√®que Spark SQL pour transformer les donn√©es en filtrant les lignes, en d√©rivant, en supprimant et en renommant des colonnes, et en appliquant d‚Äôautres modifications de donn√©es.

>[!TIP]
> Consultez la documentation des [DataFrames Spark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html) pour en savoir plus sur l‚Äôobjet Dataframe.

### Enregistrer les donn√©es transform√©es

√Ä ce stade, vous pouvez enregistrer les donn√©es transform√©es afin qu‚Äôelles puissent √™tre utilis√©es pour une analyse plus approfondie.

*Parquet* est un format de stockage de donn√©es populaire, car il stocke les donn√©es efficacement et est pris en charge par la plupart des syst√®mes d‚Äôanalytique donn√©es √† grande √©chelle. En effet, la transformation de donn√©es consiste parfois √† convertir des donn√©es d‚Äôun format comme CSV vers Parquet.

1. Pour enregistrer le DataFrame transform√© au format Parquet, ajoutez une cellule de code et ajoutez le code suivant¬†:  

```python
transformed_df.write.mode("overwrite").parquet('Files/transformed_data/orders')

print ("Transformed data saved!")
```

2. Ex√©cutez la cellule et attendez le message indiquant que les donn√©es ont √©t√© enregistr√©es. Ensuite, dans le volet Lakehouses √† gauche, dans le menu ‚Ä¶ du n≈ìud Fichiers, s√©lectionnez **Actualiser**. S√©lectionnez le dossier transformed_data pour v√©rifier qu‚Äôil contient un nouveau dossier nomm√© orders, qui contient √† son tour un ou plusieurs fichiers Parquet.

3. Ajoutez une cellule avec le code suivant¬†:

```python
orders_df = spark.read.format("parquet").load("Files/transformed_data/orders")
display(orders_df)
```

4. Ex√©cutez la cellule.  Un DataFrame est cr√©√© √† partir des fichiers Parquet dans le dossier *transformed_data/orders*. V√©rifiez que les r√©sultats affichent les donn√©es de commandes qui ont √©t√© charg√©es √† partir des fichiers Parquet.

![Image d‚Äô√©cran montrant les fichiers Parquet.](Images/parquet-files.jpg)

### Enregistrer des donn√©es dans des fichiers partitionn√©s

Lorsque vous traitez de gros volumes de donn√©es, le partitionnement peut am√©liorer consid√©rablement les performances et faciliter le filtrage des donn√©es.

1. Ajoutez une cellule avec le code pour enregistrer le DataFrame, en partitionnant les donn√©es par ann√©e et par mois¬†:

```python
orders_df.write.partitionBy("Year","Month").mode("overwrite").parquet("Files/partitioned_data")

print ("Transformed data saved!")
```

2.  Ex√©cutez la cellule et attendez le message indiquant que les donn√©es ont √©t√© enregistr√©es. Ensuite, dans le volet Lakehouses √† gauche, dans le menu ‚Ä¶ du n≈ìud Fichiers, s√©lectionnez **Actualiser**, puis d√©veloppez le dossier partitioned_orders pour v√©rifier qu‚Äôil contient une hi√©rarchie de dossiers nomm√©s *Year=xxxx*, chacun contenant des dossiers nomm√©s *Month=xxxx*. Chaque dossier de mois contient un fichier Parquet avec les commandes de ce mois.

![Image d‚Äô√©cran montrant les donn√©es partitionn√©s par ann√©e et mois.](Images/partitioned-data.jpg)

3. Ajoutez une nouvelle cellule avec le code suivant pour charger un nouveau DataFrame √† partir du fichier orders.parquet¬†:

```python
orders_2021_df = spark.read.format("parquet").load("Files/partitioned_data/Year=2021/Month=*")

display(orders_2021_df)
```

4. Ex√©cutez cette cellule et v√©rifiez que les r√©sultats affichent les donn√©es de commandes pour les ventes de 2021. Notez que les colonnes de partitionnement sp√©cifi√©es dans le chemin (Year et Month) ne sont pas incluses dans le DataFrame.

## Utiliser des tables et SQL

Vous avez d√©sormais vu comment les m√©thodes natives de l‚Äôobjet DataFrame vous permettent d‚Äôinterroger et d‚Äôanalyser des donn√©es √† partir d‚Äôun fichier. Toutefois, vous pouvez √™tre plus √† l‚Äôaise avec les tables √† l‚Äôaide de la syntaxe SQL. Spark fournit un metastore dans lequel vous pouvez d√©finir des tables relationnelles. 

La biblioth√®que Spark SQL prend en charge l‚Äôutilisation d‚Äôinstructions SQL pour interroger les tables figurant dans le metastore. Cela permet d‚Äôallier la flexibilit√© d‚Äôun lac de donn√©es au sch√©ma de donn√©es structur√© et aux requ√™tes SQL d‚Äôun entrep√¥t de donn√©es relationnelles, d‚Äôo√π le terme de ¬´¬†data lakehouse¬†¬ª.

### Cr√©er une table

Les tables d‚Äôun metastore Spark sont des abstractions relationnelles sur les fichiers figurant dans le lac de donn√©es. Les tables peuvent √™tre *manag√©es* par le metastore, ou *externes* et manag√©es ind√©pendamment du metastore.

1.  Ajoutez une cellule de code au notebook, puis entrez le code suivant, qui enregistre le DataFrame des donn√©es de commandes client sous la forme d‚Äôune table nomm√©e *salesorders*¬†:

```python
# Create a new table
df.write.format("delta").saveAsTable("salesorders")

# Get the table description
spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)
```

>[!NOTE]
> Dans cet exemple, aucun chemin explicite n‚Äôest fourni, de sorte que les fichiers de la table sont manag√©s par le metastore. En outre, la table est enregistr√©e au format delta, ce qui ajoute des fonctionnalit√©s de base de donn√©es relationnelle aux tables. Cela inclut la prise en charge des transactions, du contr√¥le de version des lignes et d‚Äôautres fonctionnalit√©s utiles. La cr√©ation de tables au format delta est recommand√©e pour les data lakehouses dans Fabric.

2. Ex√©cutez la cellule de code et passez en revue la sortie, qui d√©crit la d√©finition de la nouvelle table.

3. Dans le volet **Lakehouses**, dans le menu ‚Ä¶ du dossier Tables, s√©lectionnez **Actualiser**. D√©veloppez ensuite le n≈ìud **Tables** et v√©rifiez que la table **salesorders** a √©t√© cr√©√©e.

![Image d‚Äô√©cran montrant que la table salesorders a √©t√© cr√©√©e.](Images/salesorders-table.jpg)

4. Dans le menu ‚Ä¶ de la table salesorders, s√©lectionnez **Charger des donn√©es** > **Spark**. Une nouvelle cellule de code contenant du code similaire √† l‚Äôexemple suivant¬†:

```pyspark
df = spark.sql("SELECT * FROM [your_lakehouse].salesorders LIMIT 1000")

display(df)
```

5. Ex√©cutez le nouveau code, qui utilise la biblioth√®que Spark SQL pour incorporer une requ√™te SQL sur la table *salesorder* dans le code PySpark et charger les r√©sultats de la requ√™te dans un DataFrame.

### Ex√©cuter du code SQL dans une cellule

Bien qu‚Äôil soit utile d‚Äôincorporer des instructions SQL dans une cellule contenant du code PySpark, les analystes de donn√©es veulent souvent simplement travailler directement dans SQL.

1. Ajoutez une nouvelle cellule de code au notebook, puis entrez le code suivant¬†:

```SparkSQL
%%sql
SELECT YEAR(OrderDate) AS OrderYear,
       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue
FROM salesorders
GROUP BY YEAR(OrderDate)
ORDER BY OrderYear;
```

7. Ex√©cutez la cellule et passez en revue les r√©sultats. Observez que¬†:

* La commande **%%sql** au d√©but de la cellule (appel√©e magic) change le langage en Spark SQL au lieu de PySpark.
* Le code SQL fait r√©f√©rence √† la table *salesorders* que vous avez cr√©√©e pr√©c√©demment.
* La sortie de la requ√™te SQL s‚Äôaffiche automatiquement en tant que r√©sultat sous la cellule.

>[!NOTE]
> Pour plus d‚Äôinformations sur Spark SQL et les DataFrames, consultez la documentation [Spark SQL](https://spark.apache.org/sql/).

## Visualiser les donn√©es avec Spark

Les graphiques vous aident √† voir les mod√®les et les tendances plus rapidement que possible en analysant des milliers de lignes de donn√©es. Les notebooks Fabric incluent une vue graphique int√©gr√©e, mais elle n‚Äôest pas con√ßue pour les graphiques complexes. Pour plus de contr√¥le sur la fa√ßon dont les graphiques sont cr√©√©s √† partir de donn√©es dans des DataFrames, utilisez des biblioth√®ques graphiques Python telles que *matplotlib* ou *seaborn*.

### Afficher les r√©sultats sous forme de graphique

1. Ajoutez une nouvelle cellule de code et entrez le code suivant¬†:

```python
%%sql
SELECT * FROM salesorders
```

2. Ex√©cutez le code pour afficher les donn√©es de la vue salesorders que vous avez cr√©√©e pr√©c√©demment. Dans la section des r√©sultats sous la cellule, modifiez l‚Äôoption **Affichage** de **Tableau** √† **Graphique**.

3.  Utilisez le bouton **Personnaliser le graphique** en haut √† droite du graphique pour d√©finir les options suivantes¬†:

* Type de graphique¬†: Graphique √† barres
* Cl√©¬†: √âl√©ment
* Valeurs¬†: Quantit√©
* Groupe de s√©ries¬†: laissez vide
* Agr√©gation¬†: Somme
* Empil√©¬†: Non s√©lectionn√©

Lorsque vous avez termin√©, s√©lectionnez **Appliquer**.

4. Votre graphique doit ressembler √† ceci¬†:

![Image d‚Äô√©cran de la vue graphique de notebook Fabric.](Images/built-in-chart.jpg) 

### Bien d√©marrer avec matplotlib

1. Ajoutez une nouvelle cellule de code et entrez le code suivant¬†:

```python
sqlQuery = "SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \
                SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue \
            FROM salesorders \
            GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \
            ORDER BY OrderYear"
df_spark = spark.sql(sqlQuery)
df_spark.show()
```

2. Ex√©cutez le code. Il retourne un DataFrame Spark contenant le chiffre d‚Äôaffaires annuel. Pour visualiser les donn√©es sous forme graphique, nous allons utiliser d‚Äôabord la biblioth√®que Python matplotlib. Cette biblioth√®que est la biblioth√®que de tra√ßage principale sur laquelle de nombreuses autres biblioth√®ques sont bas√©es, et elle offre une grande flexibilit√© dans la cr√©ation de graphiques.

3. Ajoutez une nouvelle cellule de code, puis ajoutez le code suivant¬†:

```python
from matplotlib import pyplot as plt

# matplotlib requires a Pandas dataframe, not a Spark one
df_sales = df_spark.toPandas()

# Create a bar plot of revenue by year
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'])

# Display the plot
plt.show()
```

4. Ex√©cutez la cellule et passez en revue les r√©sultats, qui se composent d‚Äôun histogramme indiquant le chiffre d‚Äôaffaires brut total pour chaque ann√©e. Passez en revue le code et notez les points suivants¬†:

* La biblioth√®que matplotlib n√©cessite un DataFrame Pandas. Vous devez donc convertir le DataFrame Spark retourn√© par la requ√™te Spark SQL.
* Au c≈ìur de la biblioth√®que matplotlib figure l‚Äôobjet *pyplot*. Il s‚Äôagit de la base de la plupart des fonctionnalit√©s de tra√ßage.
* Les param√®tres par d√©faut aboutissent √† un graphique utilisable, mais il existe de nombreuses fa√ßons de le personnaliser.

5.  Modifiez le code pour tracer le graphique comme suit¬†:

```python
from matplotlib import pyplot as plt

# Clear the plot area
plt.clf()

# Create a bar plot of revenue by year
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')

# Customize the chart
plt.title('Revenue by Year')
plt.xlabel('Year')
plt.ylabel('Revenue')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.xticks(rotation=45)

# Show the figure
plt.show()
```

6. R√©ex√©cutez la cellule de code et examinez les r√©sultats. Le graphique est d√©sormais plus facile √† comprendre.
7. Un trac√© est contenu dans une figure. Dans les exemples pr√©c√©dents, la figure a √©t√© cr√©√©e implicitement, mais elle peut √™tre cr√©√©e explicitement. Modifiez le code pour tracer le graphique comme suit¬†:

```python
from matplotlib import pyplot as plt

# Clear the plot area
plt.clf()

# Create a Figure
fig = plt.figure(figsize=(8,3))

# Create a bar plot of revenue by year
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')

# Customize the chart
plt.title('Revenue by Year')
plt.xlabel('Year')
plt.ylabel('Revenue')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.xticks(rotation=45)

# Show the figure
plt.show()
```

8. R√©ex√©cutez la cellule de code et examinez les r√©sultats. La figure d√©termine la forme et la taille du trac√©.
9. Une figure peut contenir plusieurs sous-trac√©s, chacun sur son propre axe. Modifiez le code pour tracer le graphique comme suit¬†:

```python
from matplotlib import pyplot as plt

# Clear the plot area
plt.clf()

# Create a figure for 2 subplots (1 row, 2 columns)
fig, ax = plt.subplots(1, 2, figsize = (10,4))

# Create a bar plot of revenue by year on the first axis
ax[0].bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')
ax[0].set_title('Revenue by Year')

# Create a pie chart of yearly order counts on the second axis
yearly_counts = df_sales['OrderYear'].value_counts()
ax[1].pie(yearly_counts)
ax[1].set_title('Orders per Year')
ax[1].legend(yearly_counts.keys().tolist())

# Add a title to the Figure
fig.suptitle('Sales Data')

# Show the figure
plt.show()
```

10. R√©ex√©cutez la cellule de code et examinez les r√©sultats. 

>[!NOTE] 
> Pour en savoir plus sur le tra√ßage avec matplotlib, consultez la documentation [matplotlib](https://matplotlib.org/).

### Utiliser la biblioth√®que seaborn

Bien que *matplotlib* vous permette de cr√©er des types de graphiques diff√©rents, du code complexe peut √™tre n√©cessaire pour obtenir les meilleurs r√©sultats. Pour cette raison, de nombreuses nouvelles biblioth√®ques ont √©t√© construites sur matplotlib pour en extraire la complexit√© et am√©liorer ses capacit√©s. L‚Äôune de ces biblioth√®ques est seaborn.

1. Ajoutez une nouvelle cellule de code au notebook, puis entrez le code suivant¬†: 

```python
import seaborn as sns

# Clear the plot area
plt.clf()

# Create a bar chart
ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)

plt.show()
```

2. Ex√©cutez le code pour afficher un graphique √† barres cr√©√© √† l‚Äôaide de la biblioth√®que seaborn.
3. Modifiez le code comme suit¬†:

```python
import seaborn as sns

# Clear the plot area
plt.clf()

# Set the visual theme for seaborn
sns.set_theme(style="whitegrid")

# Create a bar chart
ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)

plt.show()
```

4.  Ex√©cutez le code modifi√© et notez que seaborn vous permet de d√©finir un th√®me de couleur pour vos trac√©s.
5.  Modifiez √† nouveau le code comme suit¬†:

```python
import seaborn as sns

# Clear the plot area
plt.clf()

# Create a line chart
ax = sns.lineplot(x="OrderYear", y="GrossRevenue", data=df_sales)

plt.show()
```

6.  Ex√©cutez le code modifi√© pour afficher le chiffre d‚Äôaffaires annuel sous forme de graphique en courbes.

>[!NOTE]
> Pour en savoir plus sur le tra√ßage avec seaborn, consultez la documentation [seaborn](https://seaborn.pydata.org/index.html).

## Nettoyer les ressources

Dans cet exercice, vous avez appris √† utiliser Spark pour travailler sur des donn√©es dans Microsoft¬†Fabric.

Si vous avez termin√© d‚Äôexplorer vos donn√©es, vous pouvez mettre fin √† la session Spark et supprimer l‚Äôespace de travail que vous avez cr√©√© pour cet exercice.

1.  Dans le menu du notebook, s√©lectionnez **Arr√™ter la session** pour mettre fin √† la session Spark.
1.  Dans la barre de gauche, s√©lectionnez l‚Äôic√¥ne de votre espace de travail pour afficher tous les √©l√©ments qu‚Äôil contient.
1.  S√©lectionnez **Param√®tres de l‚Äôespace de travail** et, dans la section**G√©n√©ral**, faites d√©filer vers le bas et s√©lectionnez **Supprimer cet espace de travail**.
1.  S√©lectionnez **Supprimer** pour supprimer l‚Äôespace de travail.

