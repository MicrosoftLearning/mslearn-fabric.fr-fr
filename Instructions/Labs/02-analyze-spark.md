---
lab:
  title: Analyser des donnÃ©es avec Apache Spark
  module: Use Apache Spark to work with files in a lakehouse
---

# Analyser des donnÃ©es avec Apache Spark dans Fabric

Dans ce labo, vous allez ingÃ©rer des donnÃ©es dans le lakehouse Fabric et utiliser PySpark pour lire et analyser les donnÃ©es.

Ce labo prend environ 45Â minutes.

## PrÃ©requis

* [Version dâ€™Ã©valuation de MicrosoftÂ Fabric](/fabric/get-started/fabric-trial#start-the-fabric-capacity-trial)

## CrÃ©er un espace de travail

Avant de pouvoir utiliser des donnÃ©es dans Fabric, vous devez crÃ©er un espace de travail.

1. Sur la page dâ€™accueil de [Microsoft Fabric](https://app.fabric.microsoft.com) Ã  lâ€™adresse https://app.fabric.microsoft.com, sÃ©lectionnez lâ€™expÃ©rience **Engineering donnÃ©es**.
1. Dans la barre de navigation de gauche, sÃ©lectionnez **Espaces de travail ** (ğŸ—‡), puis **Nouvel espace de travail**.
1. Nommez le nouvel espace de travail et, dans la section **AvancÃ©**, sÃ©lectionnez le mode de licence appropriÃ©. Si vous avez dÃ©marrÃ© une version dâ€™Ã©valuation de MicrosoftÂ Fabric, sÃ©lectionnez Version dâ€™Ã©valuation.
1. SÃ©lectionnez **Appliquer** pour crÃ©er et ouvrir lâ€™espace de travail.
 
![Image dâ€™Ã©cran des fichiers CSV chargÃ©s dans un nouvel espace de travail Fabric.](Images/uploaded-files.jpg)

## CrÃ©er un lakehouse et charger des fichiers

Maintenant que vous disposez dâ€™un espace de travail, vous pouvez crÃ©er un lakehouse pour vos fichiers de donnÃ©es. Dans votre nouvel espace de travail, sÃ©lectionnez **Nouveau** et **Lakehouse**. Nommez le lakehouse, puis sÃ©lectionnez **CrÃ©er**. AprÃ¨s un court dÃ©lai, un nouveau lakehouse est crÃ©Ã©.

Vous pouvez dÃ©sormais ingÃ©rer des donnÃ©es dans le lakehouse. Il existe plusieurs faÃ§ons de procÃ©der, mais vous allez pour le moment tÃ©lÃ©charger un dossier de fichiers texte de votre ordinateur local (ou machine virtuelle de labo le cas Ã©chÃ©ant), puis les charger dans votre lakehouse.

1. TÃ©lÃ©chargez tous les fichiers de donnÃ©es Ã  partir de https://github.com/MicrosoftLearning/dp-data/raw/main/orders.zip.
1. Extrayez lâ€™archive compressÃ©e et vÃ©rifiez que vous disposez dâ€™un dossier nommÃ© *orders* qui contient trois fichiersÂ CSVÂ : 2019.csv, 2020.csv et 2021.csv.
1. Revenez Ã  votre nouveau lakehouse. Dans le volet **Explorateur**, en regard du dossier **Fichiers**, sÃ©lectionnez le menu **â€¦**, puis sÃ©lectionnez **Charger** et **Charger le dossier**. AccÃ©dez au dossier orders sur votre ordinateur local (ou machine virtuelle de labo le cas Ã©chÃ©ant) et sÃ©lectionnez **Charger**.
1. Une fois les fichiers chargÃ©s, dÃ©veloppez **Files** et sÃ©lectionnez le dossier **orders**. VÃ©rifiez que les fichiers CSV ont Ã©tÃ© chargÃ©s, comme indiquÃ© iciÂ :

![Image dâ€™Ã©cran dâ€™un nouvel espace de travail Fabric.](Images/new-workspace.jpg)

## CrÃ©er un notebook

Vous pouvez maintenant crÃ©er un notebook Fabric pour utiliser vos donnÃ©es. Les notebooks fournissent un environnement interactif dans lequel vous pouvez Ã©crire et exÃ©cuter du code (dans plusieurs langues).

1. SÃ©lectionnez votre espace de travail, puis sÃ©lectionnez **Nouveau** et **Notebook**. AprÃ¨s quelques secondes, un nouveau notebook contenant une seule cellule sâ€™ouvre. Les notebooks sont constituÃ©s dâ€™une ou plusieurs cellules qui peuvent contenir du code ou du Markdown (texte mis en forme).
1. Fabric attribue un nom Ã  chaque notebook que vous crÃ©ez, tel que NotebookÂ 1, NotebookÂ 2, etc. Cliquez sur le panneau de noms au-dessus de lâ€™onglet **Accueil** du menu pour remplacer le nom par quelque chose de plus descriptif.
1. SÃ©lectionnez la premiÃ¨re cellule (qui est actuellement une cellule de code) puis, dans la barre dâ€™outils en haut Ã  droite, utilisez le bouton **Mâ†“** pour convertir la cellule en cellule Markdown. Le texte contenu dans la cellule sâ€™affiche alors sous forme de texte mis en forme.
1. Utilisez le bouton ğŸ–‰ (Modifier) pour placer la cellule en mode Ã©dition, puis modifiez le balisage Markdown comme suit.

```markdown
# Sales order data exploration
Use this notebook to explore sales order data
```
![Image dâ€™Ã©cran dâ€™un notebook Fabric avec une cellule Markdown.](Images/name-notebook-markdown.jpg)

Lorsque vous avez terminÃ©, cliquez nâ€™importe oÃ¹ dans le notebook en dehors de la cellule pour arrÃªter sa modification et voir le balisage Markdown rendu.


## CrÃ©er un DataFrame

Maintenant que vous avez crÃ©Ã© un espace de travail, un lakehouse et un notebook, vous pouvez utiliser vos donnÃ©es. Vous allez utiliser PySpark, qui est le langage par dÃ©faut pour les notebooks Fabric et la version de Python optimisÃ©e pour Spark.

**REMARQUEÂ :** les notebooks Fabric prennent en charge plusieurs langages de programmation, notamment Scala, R et Spark SQL.

1. SÃ©lectionnez votre nouvel espace de travail dans la barre de gauche. Vous verrez une liste dâ€™Ã©lÃ©ments contenus dans lâ€™espace de travail, y compris votre lakehouse et votre notebook.
2. SÃ©lectionnez le lakehouse pour afficher le volet Explorateur, y compris le dossier **orders**.
3. Dans le menu supÃ©rieur, sÃ©lectionnez **Ouvrir le notebook**, **Notebook existant**, puis ouvrez le notebook que vous avez crÃ©Ã© prÃ©cÃ©demment. Le notebook doit maintenant Ãªtre ouvert en regard du volet Explorateur. DÃ©veloppez Lakehouses, dÃ©veloppez la liste Files et sÃ©lectionnez le dossier orders. Les fichiers CSV que vous avez chargÃ©s sont rÃ©pertoriÃ©s en regard de lâ€™Ã©diteur de notebook, comme suitÂ :

![Image dâ€™Ã©cran des fichiersÂ CSV en mode Explorateur.](Images/explorer-notebook-view.jpg)

4. Dans le menu â€¦ pour 2019.csv, sÃ©lectionnez **Charger des donnÃ©es** > **Spark**. Le code suivant est automatiquement gÃ©nÃ©rÃ© dans une nouvelle cellule de codeÂ :

```python
df = spark.read.format("csv").option("header","true").load("Files/orders/2019.csv")
# df now is a Spark DataFrame containing CSV data from "Files/orders/2019.csv".
display(df)
```

**ConseilÂ :** vous pouvez masquer les volets de lâ€™explorateur Lakehouse Ã  gauche Ã  lâ€™aide des icÃ´nes Â«. Cela donne plus dâ€™espace pour le notebook.

5. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code.

**REMARQUEÂ :** la premiÃ¨re fois que vous exÃ©cutez du code Spark, une session Spark est dÃ©marrÃ©e. Cette opÃ©ration peut prendre quelques secondes, voire plus. Les exÃ©cutions suivantes dans la mÃªme session seront plus rapides.

6. Une fois le code de la cellule exÃ©cutÃ©, examinez la sortie sous la cellule, qui doit ressembler Ã  ceciÂ :
 
![Image dâ€™Ã©cran montrant le code et les donnÃ©es gÃ©nÃ©rÃ©s automatiquement.](Images/auto-generated-load.jpg)

7. La sortie affiche les donnÃ©es du fichier 2019.csv affichÃ©es en lignes et en colonnes.  Notez que les en-tÃªtes de colonne contiennent la premiÃ¨re ligne des donnÃ©es. Pour corriger ce problÃ¨me, vous devez modifier la premiÃ¨re ligne du code comme suitÂ :

```python
df = spark.read.format("csv").option("header","false").load("Files/orders/2019.csv")
```

8. RÃ©exÃ©cutez le code afin que le DataFrame identifie correctement la premiÃ¨re ligne en tant que donnÃ©es. Notez que les noms de colonnes sont dÃ©sormais _c0, _c1, etc.

9. Les noms de colonnes descriptifs vous aident Ã  comprendre les donnÃ©es. Pour crÃ©er des noms de colonnes explicites, vous devez dÃ©finir le schÃ©ma et les types de donnÃ©es. Vous devez Ã©galement importer un ensemble standard de types SPARK SQL pour dÃ©finir les types de donnÃ©es. Remplacez le code existant par le code ci-dessousÂ :

```python
from pyspark.sql.types import *

orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
    ])

df = spark.read.format("csv").schema(orderSchema).load("Files/orders/2019.csv")

display(df)

```
10. ExÃ©cutez la cellule et passez en revue la sortieÂ :

![Image dâ€™Ã©cran du code avec schÃ©ma dÃ©fini et donnÃ©es.](Images/define-schema.jpg)

11. Le DataFrame inclut uniquement les donnÃ©es du fichier 2019.csv. Modifiez le code afin que le chemin de fichier utilise un caractÃ¨re gÃ©nÃ©rique * pour lire toutes les donnÃ©es dans le dossier ordersÂ :

```python
from pyspark.sql.types import *

orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
    ])

df = spark.read.format("csv").schema(orderSchema).load("Files/orders/*.csv")

display(df)
```

12. Lorsque vous exÃ©cutez le code modifiÃ©, vous devez voir les ventes pour 2019, 2020 et 2021. Seul un sous-ensemble des lignes est affichÃ©. Vous ne pouvez donc pas voir les lignes pour chaque annÃ©e.

**REMARQUEÂ :** vous pouvez masquer ou afficher la sortie dâ€™une cellule en sÃ©lectionnant â€¦ Ã  cÃ´tÃ© du rÃ©sultat. Cela facilite lâ€™utilisation dâ€™un notebook.

## Explorer les donnÃ©es dans un DataFrame

Lâ€™objet DataFrame fournit des fonctionnalitÃ©s supplÃ©mentaires, telles que la possibilitÃ© de filtrer, de regrouper et de manipuler des donnÃ©es.

### Filtrer un DataFrame

1. Ajoutez une cellule de code en sÃ©lectionnant **+Code** qui apparaÃ®t lorsque vous pointez la souris au-dessus ou en dessous de la cellule active ou de sa sortie. Sinon, dans le menu du ruban, sÃ©lectionnez **Modifier** et **+Ajouter une cellule de code**.

2.  Le code suivant filtre les donnÃ©es afin que seules deux colonnes soient retournÃ©es. Il utilise Ã©galement *count* et *distinct* pour rÃ©sumer le nombre dâ€™enregistrementsÂ :

```python
customers = df['CustomerName', 'Email']

print(customers.count())
print(customers.distinct().count())

display(customers.distinct())
```

3. ExÃ©cutez le code et examinez la sortie.

* Le code crÃ©e un DataFrame appelÃ© **customers** qui contient un sous-ensemble de colonnes Ã  partir du DataFrame **df** dâ€™origine. Lors de lâ€™exÃ©cution dâ€™une transformation DataFrame, vous ne modifiez pas le DataFrame dâ€™origine, mais vous en retournez un nouveau.
* Une autre faÃ§on dâ€™obtenir le mÃªme rÃ©sultat consiste Ã  utiliser la mÃ©thode selectÂ :

```
customers = df.select("CustomerName", "Email")
```

* Les fonctions DataFrame * count* et *distinct* sont utilisÃ©es pour fournir des totaux pour le nombre de clients et les clients uniques.

4. Modifiez la premiÃ¨re ligne du code Ã  lâ€™aide de *select* avec une fonction *where* comme suitÂ :

```python
customers = df.select("CustomerName", "Email").where(df['Item']=='Road-250 Red, 52')
print(customers.count())
print(customers.distinct().count())

display(customers.distinct())
```

5. ExÃ©cutez ce code modifiÃ© pour sÃ©lectionner uniquement les clients qui ont achetÃ© le produit Road-250 Red, 52. Notez que vous pouvez Â«Â attacherÂ Â» plusieurs fonctions ensemble afin que la sortie dâ€™une fonction devienne lâ€™entrÃ©e de la suivante. Dans ce cas, le DataFrame crÃ©Ã© par la mÃ©thode *select* est le DataFrame source pour la mÃ©thode **where** utilisÃ©e pour appliquer des critÃ¨res de filtrage.

### AgrÃ©ger et regrouper des donnÃ©es dans un DataFrame

1. Ajoutez une cellule de code et entrez le code suivantÂ :

```python
productSales = df.select("Item", "Quantity").groupBy("Item").sum()

display(productSales)
```

2. ExÃ©cutez le code. Vous pouvez voir que les rÃ©sultats affichent la somme des quantitÃ©s de commandes regroupÃ©es par produit. La mÃ©thode *groupBy* regroupe les lignes par Item, et la fonction dâ€™agrÃ©gation *sum* suivante est appliquÃ©e Ã  toutes les colonnes numÃ©riques restantes, dans ce cas, *Quantity*.

3. Ajoutez une autre cellule de code au notebook, puis entrez le code suivantÂ :

```python
from pyspark.sql.functions import *

yearlySales = df.select(year(col("OrderDate")).alias("Year")).groupBy("Year").count().orderBy("Year")

display(yearlySales)
```

4. ExÃ©cutez la cellule. Examinez la sortie. Cette fois, les rÃ©sultats indiquent le nombre de commandes client par an.

* Lâ€™instruction *import* vous permet dâ€™utiliser la bibliothÃ¨que SPARK SQL.
* La mÃ©thode *select* est utilisÃ©e avec une fonction year SQL pour extraire le composant year du champ *OrderDate*.
* La mÃ©thode *alias* est utilisÃ©e pour affecter un nom de colonne Ã  la valeur dâ€™annÃ©e extraite.
* La mÃ©thode *groupBy* regroupe les donnÃ©es par la colonne Year dÃ©rivÃ©e.
* Le nombre de lignes dans chaque groupe est calculÃ© avant que la mÃ©thode *orderBy* soit utilisÃ©e pour trier le DataFrame rÃ©sultant.

![Image dâ€™Ã©cran montrant les rÃ©sultats de lâ€™agrÃ©gation et du regroupement de donnÃ©es dans un DataFrame.](Images/spark-sql-dataframe.jpg)

## Utiliser Spark pour transformer des fichiers de donnÃ©es

Une tÃ¢che courante des ingÃ©nieurs et scientifiques des donnÃ©es consiste Ã  transformer des donnÃ©es pour poursuivre leur traitement ou analyse en aval.

### Utiliser des mÃ©thodes et des fonctions de DataFrame pour transformer les donnÃ©es

1. Ajoutez une cellule de code au notebook, puis entrez ce qui suitÂ :

```python
from pyspark.sql.functions import *

# Create Year and Month columns
transformed_df = df.withColumn("Year", year(col("OrderDate"))).withColumn("Month", month(col("OrderDate")))

# Create the new FirstName and LastName fields
transformed_df = transformed_df.withColumn("FirstName", split(col("CustomerName"), " ").getItem(0)).withColumn("LastName", split(col("CustomerName"), " ").getItem(1))

# Filter and reorder columns
transformed_df = transformed_df["SalesOrderNumber", "SalesOrderLineNumber", "OrderDate", "Year", "Month", "FirstName", "LastName", "Email", "Item", "Quantity", "UnitPrice", "Tax"]

# Display the first five orders
display(transformed_df.limit(5))
```

2. ExÃ©cutez la cellule. Un nouveau DataFrame est crÃ©Ã© Ã  partir des donnÃ©es de commandes dâ€™origine avec les transformations suivantesÂ :

- Colonnes Year et Month ajoutÃ©es, basÃ©es sur la colonne OrderDate.
- Colonnes FirstName et LastName ajoutÃ©es, basÃ©es sur la colonne CustomerName.
- Les colonnes sont filtrÃ©es et rÃ©organisÃ©es, et la colonne CustomerName est supprimÃ©e.

3. Passez en revue la sortie et vÃ©rifiez que les transformations ont Ã©tÃ© apportÃ©es aux donnÃ©es.

Vous pouvez utiliser la bibliothÃ¨que Spark SQL pour transformer les donnÃ©es en filtrant les lignes, en dÃ©rivant, en supprimant et en renommant des colonnes, et en appliquant dâ€™autres modifications de donnÃ©es.

>[!TIP]
> Consultez la documentation des [DataFrames Spark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html) pour en savoir plus sur lâ€™objet Dataframe.

### Enregistrer les donnÃ©es transformÃ©es

Ã€ ce stade, vous pouvez enregistrer les donnÃ©es transformÃ©es afin quâ€™elles puissent Ãªtre utilisÃ©es pour une analyse plus approfondie.

*Parquet* est un format de stockage de donnÃ©es populaire, car il stocke les donnÃ©es efficacement et est pris en charge par la plupart des systÃ¨mes dâ€™analytique donnÃ©es Ã  grande Ã©chelle. En effet, la transformation de donnÃ©es consiste parfois Ã  convertir des donnÃ©es dâ€™un format comme CSV vers Parquet.

1. Pour enregistrer le DataFrame transformÃ© au format Parquet, ajoutez une cellule de code et ajoutez le code suivantÂ :  

```python
transformed_df.write.mode("overwrite").parquet('Files/transformed_data/orders')

print ("Transformed data saved!")
```

2. ExÃ©cutez la cellule et attendez le message indiquant que les donnÃ©es ont Ã©tÃ© enregistrÃ©es. Ensuite, dans le volet Lakehouses Ã  gauche, dans le menu â€¦ du nÅ“ud Fichiers, sÃ©lectionnez **Actualiser**. SÃ©lectionnez le dossier transformed_data pour vÃ©rifier quâ€™il contient un nouveau dossier nommÃ© orders, qui contient Ã  son tour un ou plusieurs fichiers Parquet.

3. Ajoutez une cellule avec le code suivantÂ :

```python
orders_df = spark.read.format("parquet").load("Files/transformed_data/orders")
display(orders_df)
```

4. ExÃ©cutez la cellule.  Un DataFrame est crÃ©Ã© Ã  partir des fichiers Parquet dans le dossier *transformed_data/orders*. VÃ©rifiez que les rÃ©sultats affichent les donnÃ©es de commandes qui ont Ã©tÃ© chargÃ©es Ã  partir des fichiers Parquet.

![Image dâ€™Ã©cran montrant les fichiers Parquet.](Images/parquet-files.jpg)

### Enregistrer des donnÃ©es dans des fichiers partitionnÃ©s

Lorsque vous traitez de gros volumes de donnÃ©es, le partitionnement peut amÃ©liorer considÃ©rablement les performances et faciliter le filtrage des donnÃ©es.

1. Ajoutez une cellule avec le code pour enregistrer le DataFrame, en partitionnant les donnÃ©es par annÃ©e et par moisÂ :

```python
orders_df.write.partitionBy("Year","Month").mode("overwrite").parquet("Files/partitioned_data")

print ("Transformed data saved!")
```

2.  ExÃ©cutez la cellule et attendez le message indiquant que les donnÃ©es ont Ã©tÃ© enregistrÃ©es. Ensuite, dans le volet Lakehouses Ã  gauche, dans le menu â€¦ du nÅ“ud Fichiers, sÃ©lectionnez **Actualiser**, puis dÃ©veloppez le dossier partitioned_orders pour vÃ©rifier quâ€™il contient une hiÃ©rarchie de dossiers nommÃ©s *Year=xxxx*, chacun contenant des dossiers nommÃ©s *Month=xxxx*. Chaque dossier de mois contient un fichier Parquet avec les commandes de ce mois.

![Image dâ€™Ã©cran montrant les donnÃ©es partitionnÃ©s par annÃ©e et mois.](Images/partitioned-data.jpg)

3. Ajoutez une nouvelle cellule avec le code suivant pour charger un nouveau DataFrame Ã  partir du fichier orders.parquetÂ :

```python
orders_2021_df = spark.read.format("parquet").load("Files/partitioned_data/Year=2021/Month=*")

display(orders_2021_df)
```

4. ExÃ©cutez cette cellule et vÃ©rifiez que les rÃ©sultats affichent les donnÃ©es de commandes pour les ventes de 2021. Notez que les colonnes de partitionnement spÃ©cifiÃ©es dans le chemin (Year et Month) ne sont pas incluses dans le DataFrame.

## Utiliser des tables et SQL

Vous avez dÃ©sormais vu comment les mÃ©thodes natives de lâ€™objet DataFrame vous permettent dâ€™interroger et dâ€™analyser des donnÃ©es Ã  partir dâ€™un fichier. Toutefois, vous pouvez Ãªtre plus Ã  lâ€™aise avec les tables Ã  lâ€™aide de la syntaxe SQL. Spark fournit un metastore dans lequel vous pouvez dÃ©finir des tables relationnelles. 

La bibliothÃ¨que Spark SQL prend en charge lâ€™utilisation dâ€™instructions SQL pour interroger les tables figurant dans le metastore. Cela permet dâ€™allier la flexibilitÃ© dâ€™un lac de donnÃ©es au schÃ©ma de donnÃ©es structurÃ© et aux requÃªtes SQL dâ€™un entrepÃ´t de donnÃ©es relationnelles, dâ€™oÃ¹ le terme de Â«Â data lakehouseÂ Â».

### CrÃ©er une table

Les tables dâ€™un metastore Spark sont des abstractions relationnelles sur les fichiers figurant dans le lac de donnÃ©es. Les tables peuvent Ãªtre *managÃ©es* par le metastore, ou *externes* et managÃ©es indÃ©pendamment du metastore.

1.  Ajoutez une cellule de code au notebook, puis entrez le code suivant, qui enregistre le DataFrame des donnÃ©es de commandes client sous la forme dâ€™une table nommÃ©e *salesorders*Â :

```python
# Create a new table
df.write.format("delta").saveAsTable("salesorders")

# Get the table description
spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)
```

>[!NOTE]
> Dans cet exemple, aucun chemin explicite nâ€™est fourni, de sorte que les fichiers de la table sont managÃ©s par le metastore. En outre, la table est enregistrÃ©e au format delta, ce qui ajoute des fonctionnalitÃ©s de base de donnÃ©es relationnelle aux tables. Cela inclut la prise en charge des transactions, du contrÃ´le de version des lignes et dâ€™autres fonctionnalitÃ©s utiles. La crÃ©ation de tables au format delta est recommandÃ©e pour les data lakehouses dans Fabric.

2. ExÃ©cutez la cellule de code et passez en revue la sortie, qui dÃ©crit la dÃ©finition de la nouvelle table.

3. Dans le volet **Lakehouses**, dans le menu â€¦ du dossier Tables, sÃ©lectionnez **Actualiser**. DÃ©veloppez ensuite le nÅ“ud **Tables** et vÃ©rifiez que la table **salesorders** a Ã©tÃ© crÃ©Ã©e.

![Image dâ€™Ã©cran montrant que la table salesorders a Ã©tÃ© crÃ©Ã©e.](Images/salesorders-table.jpg)

4. Dans le menu â€¦ de la table salesorders, sÃ©lectionnez **Charger des donnÃ©es** > **Spark**. Une nouvelle cellule de code contenant du code similaire Ã  lâ€™exemple suivantÂ :

```pyspark
df = spark.sql("SELECT * FROM [your_lakehouse].salesorders LIMIT 1000")

display(df)
```

5. ExÃ©cutez le nouveau code, qui utilise la bibliothÃ¨que Spark SQL pour incorporer une requÃªte SQL sur la table *salesorder* dans le code PySpark et charger les rÃ©sultats de la requÃªte dans un DataFrame.

### ExÃ©cuter du code SQL dans une cellule

Bien quâ€™il soit utile dâ€™incorporer des instructions SQL dans une cellule contenant du code PySpark, les analystes de donnÃ©es veulent souvent simplement travailler directement dans SQL.

1. Ajoutez une nouvelle cellule de code au notebook, puis entrez le code suivantÂ :

```SparkSQL
%%sql
SELECT YEAR(OrderDate) AS OrderYear,
       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue
FROM salesorders
GROUP BY YEAR(OrderDate)
ORDER BY OrderYear;
```

7. ExÃ©cutez la cellule et passez en revue les rÃ©sultats. Observez queÂ :

* La commande **%%sql** au dÃ©but de la cellule (appelÃ©e magic) change le langage en Spark SQL au lieu de PySpark.
* Le code SQL fait rÃ©fÃ©rence Ã  la table *salesorders* que vous avez crÃ©Ã©e prÃ©cÃ©demment.
* La sortie de la requÃªte SQL sâ€™affiche automatiquement en tant que rÃ©sultat sous la cellule.

>[!NOTE]
> Pour plus dâ€™informations sur Spark SQL et les DataFrames, consultez la documentation [Spark SQL](https://spark.apache.org/sql/).

## Visualiser les donnÃ©es avec Spark

Les graphiques vous aident Ã  voir les modÃ¨les et les tendances plus rapidement que possible en analysant des milliers de lignes de donnÃ©es. Les notebooks Fabric incluent une vue graphique intÃ©grÃ©e, mais elle nâ€™est pas conÃ§ue pour les graphiques complexes. Pour plus de contrÃ´le sur la faÃ§on dont les graphiques sont crÃ©Ã©s Ã  partir de donnÃ©es dans des DataFrames, utilisez des bibliothÃ¨ques graphiques Python telles que *matplotlib* ou *seaborn*.

### Afficher les rÃ©sultats sous forme de graphique

1. Ajoutez une nouvelle cellule de code et entrez le code suivantÂ :

```python
%%sql
SELECT * FROM salesorders
```

2. ExÃ©cutez le code pour afficher les donnÃ©es de la vue salesorders que vous avez crÃ©Ã©e prÃ©cÃ©demment. Dans la section des rÃ©sultats sous la cellule, modifiez lâ€™option **Affichage** de **Tableau** Ã  **Graphique**.

3.  Utilisez le bouton **Personnaliser le graphique** en haut Ã  droite du graphique pour dÃ©finir les options suivantesÂ :

* Type de graphiqueÂ : Graphique Ã  barres
* ClÃ©Â : Ã‰lÃ©ment
* ValeursÂ : QuantitÃ©
* Groupe de sÃ©riesÂ : laissez vide
* AgrÃ©gationÂ : Somme
* EmpilÃ©Â : Non sÃ©lectionnÃ©

Lorsque vous avez terminÃ©, sÃ©lectionnez **Appliquer**.

4. Votre graphique doit ressembler Ã  ceciÂ :

![Image dâ€™Ã©cran de la vue graphique de notebook Fabric.](Images/built-in-chart.jpg) 

### Bien dÃ©marrer avec matplotlib

1. Ajoutez une nouvelle cellule de code et entrez le code suivantÂ :

```python
sqlQuery = "SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \
                SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue \
            FROM salesorders \
            GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \
            ORDER BY OrderYear"
df_spark = spark.sql(sqlQuery)
df_spark.show()
```

2. ExÃ©cutez le code. Il retourne un DataFrame Spark contenant le chiffre dâ€™affaires annuel. Pour visualiser les donnÃ©es sous forme graphique, nous allons utiliser dâ€™abord la bibliothÃ¨que Python matplotlib. Cette bibliothÃ¨que est la bibliothÃ¨que de traÃ§age principale sur laquelle de nombreuses autres bibliothÃ¨ques sont basÃ©es, et elle offre une grande flexibilitÃ© dans la crÃ©ation de graphiques.

3. Ajoutez une nouvelle cellule de code, puis ajoutez le code suivantÂ :

```python
from matplotlib import pyplot as plt

# matplotlib requires a Pandas dataframe, not a Spark one
df_sales = df_spark.toPandas()

# Create a bar plot of revenue by year
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'])

# Display the plot
plt.show()
```

4. ExÃ©cutez la cellule et passez en revue les rÃ©sultats, qui se composent dâ€™un histogramme indiquant le chiffre dâ€™affaires brut total pour chaque annÃ©e. Passez en revue le code et notez les points suivantsÂ :

* La bibliothÃ¨que matplotlib nÃ©cessite un DataFrame Pandas. Vous devez donc convertir le DataFrame Spark retournÃ© par la requÃªte Spark SQL.
* Au cÅ“ur de la bibliothÃ¨que matplotlib figure lâ€™objet *pyplot*. Il sâ€™agit de la base de la plupart des fonctionnalitÃ©s de traÃ§age.
* Les paramÃ¨tres par dÃ©faut aboutissent Ã  un graphique utilisable, mais il existe de nombreuses faÃ§ons de le personnaliser.

5.  Modifiez le code pour tracer le graphique comme suitÂ :

```python
from matplotlib import pyplot as plt

# Clear the plot area
plt.clf()

# Create a bar plot of revenue by year
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')

# Customize the chart
plt.title('Revenue by Year')
plt.xlabel('Year')
plt.ylabel('Revenue')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.xticks(rotation=45)

# Show the figure
plt.show()
```

6. RÃ©exÃ©cutez la cellule de code et examinez les rÃ©sultats. Le graphique est dÃ©sormais plus facile Ã  comprendre.
7. Un tracÃ© est contenu dans une figure. Dans les exemples prÃ©cÃ©dents, la figure a Ã©tÃ© crÃ©Ã©e implicitement, mais elle peut Ãªtre crÃ©Ã©e explicitement. Modifiez le code pour tracer le graphique comme suitÂ :

```python
from matplotlib import pyplot as plt

# Clear the plot area
plt.clf()

# Create a Figure
fig = plt.figure(figsize=(8,3))

# Create a bar plot of revenue by year
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')

# Customize the chart
plt.title('Revenue by Year')
plt.xlabel('Year')
plt.ylabel('Revenue')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.xticks(rotation=45)

# Show the figure
plt.show()
```

8. RÃ©exÃ©cutez la cellule de code et examinez les rÃ©sultats. La figure dÃ©termine la forme et la taille du tracÃ©.
9. Une figure peut contenir plusieurs sous-tracÃ©s, chacun sur son propre axe. Modifiez le code pour tracer le graphique comme suitÂ :

```python
from matplotlib import pyplot as plt

# Clear the plot area
plt.clf()

# Create a figure for 2 subplots (1 row, 2 columns)
fig, ax = plt.subplots(1, 2, figsize = (10,4))

# Create a bar plot of revenue by year on the first axis
ax[0].bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')
ax[0].set_title('Revenue by Year')

# Create a pie chart of yearly order counts on the second axis
yearly_counts = df_sales['OrderYear'].value_counts()
ax[1].pie(yearly_counts)
ax[1].set_title('Orders per Year')
ax[1].legend(yearly_counts.keys().tolist())

# Add a title to the Figure
fig.suptitle('Sales Data')

# Show the figure
plt.show()
```

10. RÃ©exÃ©cutez la cellule de code et examinez les rÃ©sultats. 

>[!NOTE] 
> Pour en savoir plus sur le traÃ§age avec matplotlib, consultez la documentation [matplotlib](https://matplotlib.org/).

### Utiliser la bibliothÃ¨que seaborn

Bien que *matplotlib* vous permette de crÃ©er des types de graphiques diffÃ©rents, du code complexe peut Ãªtre nÃ©cessaire pour obtenir les meilleurs rÃ©sultats. Pour cette raison, de nombreuses nouvelles bibliothÃ¨ques ont Ã©tÃ© construites sur matplotlib pour en extraire la complexitÃ© et amÃ©liorer ses capacitÃ©s. Lâ€™une de ces bibliothÃ¨ques est seaborn.

1. Ajoutez une nouvelle cellule de code au notebook, puis entrez le code suivantÂ : 

```python
import seaborn as sns

# Clear the plot area
plt.clf()

# Create a bar chart
ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)

plt.show()
```

2. ExÃ©cutez le code pour afficher un graphique Ã  barres crÃ©Ã© Ã  lâ€™aide de la bibliothÃ¨que seaborn.
3. Modifiez le code comme suitÂ :

```python
import seaborn as sns

# Clear the plot area
plt.clf()

# Set the visual theme for seaborn
sns.set_theme(style="whitegrid")

# Create a bar chart
ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)

plt.show()
```

4.  ExÃ©cutez le code modifiÃ© et notez que seaborn vous permet de dÃ©finir un thÃ¨me de couleur pour vos tracÃ©s.
5.  Modifiez Ã  nouveau le code comme suitÂ :

```python
import seaborn as sns

# Clear the plot area
plt.clf()

# Create a line chart
ax = sns.lineplot(x="OrderYear", y="GrossRevenue", data=df_sales)

plt.show()
```

6.  ExÃ©cutez le code modifiÃ© pour afficher le chiffre dâ€™affaires annuel sous forme de graphique en courbes.

>[!NOTE]
> Pour en savoir plus sur le traÃ§age avec seaborn, consultez la documentation [seaborn](https://seaborn.pydata.org/index.html).

## Nettoyer les ressources

Dans cet exercice, vous avez appris Ã  utiliser Spark pour travailler sur des donnÃ©es dans MicrosoftÂ Fabric.

Si vous avez terminÃ© dâ€™explorer vos donnÃ©es, vous pouvez mettre fin Ã  la session Spark et supprimer lâ€™espace de travail que vous avez crÃ©Ã© pour cet exercice.

1.  Dans le menu du notebook, sÃ©lectionnez **ArrÃªter la session** pour mettre fin Ã  la session Spark.
1.  Dans la barre de gauche, sÃ©lectionnez lâ€™icÃ´ne de votre espace de travail pour afficher tous les Ã©lÃ©ments quâ€™il contient.
1.  SÃ©lectionnez **ParamÃ¨tres de lâ€™espace de travail** et, dans la section**GÃ©nÃ©ral**, faites dÃ©filer vers le bas et sÃ©lectionnez **Supprimer cet espace de travail**.
1.  SÃ©lectionnez **Supprimer** pour supprimer lâ€™espace de travail.

