---
lab:
  title: Analysez les donnÃ©es avec Apache Spark et Copilot dans les notebooks Microsoft Fabric
  module: Get started with Copilot in Fabric for data engineering
---

# Analysez les donnÃ©es avec Apache Spark et Copilot dans les notebooks Microsoft Fabric

Dans ce labo, nous utilisons Copilot pour Fabric Data Engineering afin de charger, transformer et enregistrer des donnÃ©es dans un Lakehouse, Ã  lâ€™aide dâ€™un notebook. Les notebooks offrent un environnement interactif qui combine code, visualisations et texte narratif dans un seul document. Ce format facilite la documentation de votre flux de travail, lâ€™explication de votre raisonnement et le partage des rÃ©sultats avec dâ€™autres personnes. En utilisant des notebooks, vous pouvez dÃ©velopper et tester votre code de maniÃ¨re itÃ©rative, visualiser les donnÃ©es Ã  chaque Ã©tape et conserver une trace claire de votre processus dâ€™analyse. Cette approche favorise la collaboration, la reproductibilitÃ© et la comprÃ©hension, faisant des notebooks un outil idÃ©al pour les tÃ¢ches dâ€™ingÃ©nierie et dâ€™analyse de donnÃ©es.

Traditionnellement, travailler avec des notebooks pour lâ€™ingÃ©nierie des donnÃ©es nÃ©cessite dâ€™Ã©crire du code dans des langages tels que Python ou Scala et dâ€™avoir une bonne comprÃ©hension des infrastructures et des bibliothÃ¨ques comme Apache Spark et pandas. Cela peut sâ€™avÃ©rer difficile pour ceux qui dÃ©butent en programmation ou qui ne sont pas familiarisÃ©s avec ces outils. Avec Copilot dans les notebooks Fabric, vous pouvez dÃ©crire vos tÃ¢ches de donnÃ©es en langage naturel, et Copilot gÃ©nÃ©rera le code nÃ©cessaire pour vous, en gÃ©rant une grande partie de la complexitÃ© technique et vous permettant de vous concentrer sur votre analyse.

Cet exercice devrait prendre environ **30** minutes.

## Ce que vous allez apprendre

En suivant ce labo, vous serez capable deÂ :

- CrÃ©er et configurer un espace de travail Microsoft Fabric Lakehouse pour les tÃ¢ches dâ€™ingÃ©nierie des donnÃ©es.
- Utiliser Copilot dans les notebooks Fabric pour gÃ©nÃ©rer du code Ã  partir de prompts en langage naturel.
- IngÃ©rer, nettoyer et transformer des donnÃ©es Ã  lâ€™aide dâ€™Apache Spark et de flux de travail assistÃ©s par Copilot.
- Normaliser et prÃ©parer des jeux de donnÃ©es statistiques pour lâ€™analyse en divisant, filtrant et convertissant les types de donnÃ©es.
- Enregistrer les donnÃ©es transformÃ©es sous forme de table dans le lakehouse pour des analyses en aval.
- Utiliser Copilot pour gÃ©nÃ©rer des requÃªtes et des visualisations pour lâ€™exploration et la validation des donnÃ©es.
- Comprendre les meilleures pratiques en matiÃ¨re de nettoyage, de transformation et dâ€™analyse collaborative des donnÃ©es dans Microsoft Fabric.

## Avant de commencer

Vous devez disposer dâ€™une [capacitÃ© Microsoft Fabric (F2 ou supÃ©rieure)](https://learn.microsoft.com/fabric/fundamentals/copilot-enable-fabric) avec Copilot activÃ© pour effectuer cet exercice.

> **Remarque**Â : Pour vous faciliter la tÃ¢che, un cahier contenant tous les prompts relatifs Ã  cet exercice est disponible en tÃ©lÃ©chargement Ã  lâ€™adresse suivanteÂ :

`https://github.com/MicrosoftLearning/mslearn-fabric/raw/refs/heads/main/Allfiles/Labs/22b/Starter/eurostat-notebook.ipynb`

## ScÃ©nario de lâ€™exercice

Imaginons que Contoso Health, un rÃ©seau hospitalier Ã  spÃ©cialitÃ©s multiples, souhaite dÃ©velopper ses services dans lâ€™Union europÃ©enne et analyser les donnÃ©es dÃ©mographiques prÃ©visionnelles. Cet exemple utilise le jeu de donnÃ©es sur les projections dÃ©mographiques [Eurostat](https://ec.europa.eu/eurostat/web/main/home) (office statistique de lâ€™Union europÃ©enne).

SourceÂ : EUROPOP2023 Population au 1erÂ janvier par Ã¢ge, sexe et type de projection [[proj_23np](https://ec.europa.eu/eurostat/databrowser/product/view/proj_23np?category=proj.proj_23n)], derniÃ¨re mise Ã  jour le 28Â juinÂ 2023.

## CrÃ©er un espace de travail

Avant de travailler avec des donnÃ©es dans Fabric, crÃ©ez un espace de travail avec Fabric activÃ©. Un espace de travail dans Microsoft Fabric sert dâ€™environnement collaboratif oÃ¹ vous pouvez organiser et gÃ©rer tous vos artefacts dâ€™ingÃ©nierie des donnÃ©es, y compris les lakehouses, les notebooks et les jeux de donnÃ©es. ConsidÃ©rez-le comme un dossier de projet contenant toutes les ressources nÃ©cessaires Ã  votre analyse de donnÃ©es.

1. AccÃ©dez Ã  la [page dâ€™accueil de MicrosoftÂ Fabric](https://app.fabric.microsoft.com/home?experience=fabric) sur `https://app.fabric.microsoft.com/home?experience=fabric` dans un navigateur et connectez-vous avec vos informations dâ€™identification Fabric.

1. Dans la barre de menus Ã  gauche, sÃ©lectionnez **Espaces de travail** (lâ€™icÃ´ne ressemble Ã  &#128455;).

1. CrÃ©ez un nouvel espace de travail avec le nom de votre choix, en sÃ©lectionnant un mode de licence qui inclut la capacitÃ© Fabric (*Premium* ou *Fabric*). RemarqueÂ : *Trial* nâ€™est pas pris en charge.
   
    > **Pourquoi câ€™est important**Â : Copilot nÃ©cessite une capacitÃ© Fabric payante pour fonctionner. Cela vous garantit lâ€™accÃ¨s aux fonctionnalitÃ©s basÃ©es sur lâ€™intelligence artificielle qui vous aideront Ã  gÃ©nÃ©rer du code tout au long de ce labo.

1. Lorsque votre nouvel espace de travail sâ€™ouvre, il doit Ãªtre vide.

    ![Capture dâ€™Ã©cran dâ€™un espace de travail vide dans Fabric.](./Images/new-workspace.png)

## CrÃ©er un lakehouse

Maintenant que vous disposez dâ€™un espace de travail, il est temps de crÃ©er un lakehouse dans lequel vous allez ingÃ©rer des donnÃ©es. Un lakehouse combine les avantages dâ€™un lac de donnÃ©es (stockage de donnÃ©es brutes dans divers formats) et dâ€™un entrepÃ´t de donnÃ©es (donnÃ©es structurÃ©es optimisÃ©es pour lâ€™analyse). Il servira Ã  la fois de lieu de stockage pour nos donnÃ©es dÃ©mographiques brutes et de destination pour notre jeu de donnÃ©es nettoyÃ© et transformÃ©.

1. SÃ©lectionnez **CrÃ©er** dans la barre de menus de gauche. Dans la page *Nouveau*, sous la section *Engineering donnÃ©es*, sÃ©lectionnez **Lakehouse**. Donnez-lui un nom unique de votre choix.

    >**Note**Â : si lâ€™option **CrÃ©er** nâ€™est pas Ã©pinglÃ©e Ã  la barre latÃ©rale, vous devez dâ€™abord sÃ©lectionner lâ€™option avec des points de suspension (**...**).

![Capture dâ€™Ã©cran du bouton CrÃ©er dans Fabric.](./Images/copilot-fabric-notebook-create.png)

Au bout dâ€™une minute environ, un nouveau lakehouse vide est crÃ©Ã©.

![Capture dâ€™Ã©cran dâ€™un nouveau lakehouse.](./Images/new-lakehouse.png)

## CrÃ©er un notebook

Vous pouvez maintenant crÃ©er un notebook Fabric pour utiliser vos donnÃ©es. Les notebooks offrent un environnement interactif dans lequel vous pouvez Ã©crire et exÃ©cuter du code, visualiser les rÃ©sultats et documenter votre processus dâ€™analyse des donnÃ©es. Ils sont parfaits pour lâ€™analyse exploratoire des donnÃ©es et le dÃ©veloppement itÃ©ratif, car ils vous permettent de voir immÃ©diatement les rÃ©sultats de chaque Ã©tape.

1. SÃ©lectionnez **CrÃ©er** dans la barre de menus de gauche. Sur la page *Nouveau*, sous la section *Engineering donnÃ©es*, sÃ©lectionnez **Notebook**.

    Un notebook nommÃ© **NotebookÂ 1** est crÃ©Ã© et ouvert.

    ![Capture dâ€™Ã©cran dâ€™un nouveau notebook.](./Images/new-notebook.png)

1. Fabric attribue un nom Ã  chaque notebook que vous crÃ©ez, tel que NotebookÂ 1, NotebookÂ 2, etc. Cliquez sur le panneau de noms au-dessus de lâ€™onglet **Accueil** du menu pour remplacer le nom par quelque chose de plus descriptif.

    ![Capture dâ€™Ã©cran dâ€™un nouveau notebook, avec possibilitÃ© de renommer.](./Images/copilot-fabric-notebook-rename.png)

1. SÃ©lectionnez la premiÃ¨re cellule (qui est actuellement une cellule de code) puis, dans la barre dâ€™outils en haut Ã  droite, utilisez le bouton **Mâ†“** pour convertir la cellule en cellule Markdown. Le texte contenu dans la cellule sâ€™affiche alors sous forme de texte mis en forme.

    > **Pourquoi utiliser des cellules Markdown**Â : Les cellules Markdown vous permettent de documenter votre analyse avec du texte formatÃ©, rendant votre bloc-notes plus lisible et plus facile Ã  comprendre pour les autres (ou pour vous-mÃªme lorsque vous y reviendrez ultÃ©rieurement).

    ![Capture dâ€™Ã©cran dâ€™un notebook, modification de la premiÃ¨re cellule pour la convertir en Markdown.](./Images/copilot-fabric-notebook-markdown.png)

1. Utilisez le bouton ðŸ–‰ (Modifier) pour placer la cellule en mode Ã©dition, puis modifiez le balisage Markdown comme suit.

    ```md
    # Explore Eurostat population data.
    Use this notebook to explore population data from Eurostat
    ```
    
    ![Image dâ€™Ã©cran dâ€™un notebook Fabric avec une cellule Markdown.](Images/copilot-fabric-notebook-step-1-created.png)
    
    Lorsque vous avez terminÃ©, cliquez nâ€™importe oÃ¹ dans le notebook en dehors de la cellule pour arrÃªter la modification.

## Connectez le lakehouse Ã  votre notebook

Pour travailler avec les donnÃ©es de votre lakehouse Ã  partir du notebook, vous devez connecter le lakehouse Ã  votre notebook. Cette connexion permet Ã  votre notebook de lire et dâ€™Ã©crire dans le stockage lakehouse, crÃ©ant ainsi une intÃ©gration transparente entre votre environnement dâ€™analyse et votre stockage de donnÃ©es.

1. SÃ©lectionnez votre nouvel espace de travail dans la barre de gauche. Vous verrez une liste dâ€™Ã©lÃ©ments contenus dans lâ€™espace de travail, y compris votre lakehouse et votre notebook.

1. SÃ©lectionnez le lakehouse pour afficher le volet Explorateur.

1. Dans le menu supÃ©rieur, sÃ©lectionnez **Ouvrir le notebook**, **Notebook existant**, puis ouvrez le notebook que vous avez crÃ©Ã© prÃ©cÃ©demment. Le notebook doit maintenant Ãªtre ouvert en regard du volet Explorateur. DÃ©veloppez Lakehouses, puis la liste Fichiers. Vous remarquerez quâ€™aucune table ni aucun fichier nâ€™apparaÃ®t encore Ã  cÃ´tÃ© de lâ€™Ã©diteur de notebook, comme ceciÂ :

    ![Image dâ€™Ã©cran des fichiersÂ CSV en mode Explorateur.](Images/copilot-fabric-notebook-step-2-lakehouse-attached.png)

    > **Ce que vous voyez**Â : Le volet Explorateur Ã  gauche affiche la structure de votre lakehouse. Pour lâ€™instant, il est vide, mais Ã  mesure que nous chargerons et traiterons les donnÃ©es, vous verrez apparaÃ®tre des fichiers dans la section **Fichiers** et des tables dans la section **Tables**.


## Charger les donnÃ©es

Nous allons maintenant utiliser Copilot pour nous aider Ã  tÃ©lÃ©charger les donnÃ©es de lâ€™API Eurostat. Au lieu dâ€™Ã©crire du code Python Ã  partir de zÃ©ro, nous allons dÃ©crire ce que nous voulons faire en langage naturel, et Copilot gÃ©nÃ©rera le code appropriÃ©. Cela illustre lâ€™un des principaux avantages du codage assistÃ© par lâ€™IAÂ : vous pouvez vous concentrer sur la logique mÃ©tier plutÃ´t que sur les dÃ©tails techniques de lâ€™implÃ©mentation.

1. CrÃ©ez une nouvelle cellule dans votre notebook et copiez-y lâ€™instruction suivante. Pour indiquer que nous voulons que Copilot gÃ©nÃ¨re du code, utilisez `%%code` comme premiÃ¨re instruction dans la cellule. 

    > **Ã€ propos de la `%%code` commande magique**Â : Cette instruction spÃ©ciale indique Ã  Copilot que vous souhaitez quâ€™il gÃ©nÃ¨re du code Python Ã  partir de votre description en langage naturel. Il sâ€™agit de lâ€™une des nombreuses Â«Â commandes magiquesÂ Â» qui vous aident Ã  interagir plus efficacement avec Copilot.

    ```copilot-prompt
    %%code
    
    Download the following file from this URL:
    
    https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/proj_23np$defaultview/?format=TSV
     
    Then write the file to the default lakehouse into a folder named temp. Create the folder if it doesn't exist yet.
    ```
    
1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code.

    Copilot gÃ©nÃ¨re le code suivant, qui peut varier lÃ©gÃ¨rement en fonction de votre environnement et des derniÃ¨res mises Ã  jour de Copilot.
    
    ![Capture dâ€™Ã©cran du code gÃ©nÃ©rÃ© par Copilot.](Images/copilot-fabric-notebook-step-3-code-magic.png)
    
    > **Fonctionnement de Copilot**Â : Remarquez comment Copilot traduit votre requÃªte en langage naturel en code Python fonctionnel. Il comprend que vous devez effectuer une requÃªte HTTP, gÃ©rer le systÃ¨me de fichiers et enregistrer les donnÃ©es Ã  un emplacement prÃ©cis dans votre lakehouse.
    
    Voici le code complet pour votre commoditÃ©, au cas oÃ¹ vous rencontreriez des exceptions lors de lâ€™exÃ©cutionÂ :
    
    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    import requests
    import os
    
    # Define the URL and the local path
    url = "https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/proj_23np$defaultview/?format=TSV"
    local_path = "/lakehouse/default/Files/temp/"
    file_name = "proj_23np.tsv"
    file_path = os.path.join(local_path, file_name)
    
    # Create the temporary directory if it doesn't exist
    if not os.path.exists(local_path):
        os.makedirs(local_path)
    
    # Download the file
    response = requests.get(url)
    response.raise_for_status()  # Check that the request was successful
    
    # Write the content to the file
    with open(file_path, "wb") as file:
        file.write(response.content)
    
    print(f"File downloaded and saved to {file_path}")
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code et observer la sortie. Le fichier doit Ãªtre tÃ©lÃ©chargÃ© et enregistrÃ© dans le dossier temporaire de votre Lakehouse.

    > **Note**Â : vous devrez peut-Ãªtre actualiser vos fichiers Lakehouse en sÃ©lectionnant les trois points ...
    
    ![Capture dâ€™Ã©cran dâ€™un fichier temporaire crÃ©Ã© dans le lakehouse.](Images/copilot-fabric-notebook-step-4-lakehouse-refreshed.png)

1. Maintenant que nous avons le fichier de donnÃ©es brutes dans notre lakehouse, nous devons le charger dans un Spark DataFrame afin de pouvoir lâ€™analyser et le transformer. CrÃ©ez une nouvelle cellule dans votre notebook et copiez-y lâ€™instruction suivante.

    > **Informations**Â : Un DataFrame est un ensemble distribuÃ© de donnÃ©es organisÃ©es en colonnes nommÃ©es, similaire Ã  une table dans une base de donnÃ©es ou une feuille de calcul.

    ```copilot-prompt
    %%code
    
    Load the file 'Files/temp/proj_23np.tsv' into a spark dataframe.
    
    The fields are separated with a tab.
    
    Show the contents of the DataFrame using display method.
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code et observer la sortie. Le dataframe doit contenir les donnÃ©es du fichier TSV. Voici un exemple de ce Ã  quoi pourrait ressembler le code gÃ©nÃ©rÃ©Â :

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Load the file 'Files/temp/proj_23np.tsv' into a spark dataframe.
    # The fields have been separated with a tab.
    file_path = "Files/temp/proj_23np.tsv"
    
    spark_df = spark.read.format("csv").option("delimiter", "\t").option("header", "true").load(file_path)
    
    # Show the contents of the DataFrame using display method
    display(spark_df)
    ```

Voici un exemple de ce Ã  quoi la sortie peut ressemblerÂ :

| freq,projection,sex,age,unit,geo\TIME_PERIOD |      2022  |      2023  |   ...  |      2100  |
| -------------------------------------------- | ---------- | ---------- | ------ | ---------- |
|                         A,BSL,F,TOTAL,PER,AT |   4553444  |   4619179  |   ...  |   4807661  |
|                         A,BSL,F,TOTAL,PER,BE |   5883978  |   5947528  |   ...  |   6331785  |
|                         A,BSL,F,TOTAL,PER,BG |   3527626  |   3605059  |   ...  |   2543673  |
|                                          ... |       ...  |       ...  |   ...  |   5081250  |
|                         A,BSL,F,TOTAL,PER,CY |    463622  |    476907  |   ...  |    504781  |

> **ComprÃ©hension de la structure des donnÃ©es**Â : Notez que la premiÃ¨re colonne contient plusieurs valeurs sÃ©parÃ©es par des virgules (frÃ©quence, type de projection, sexe, Ã¢ge, unitÃ© et situation gÃ©ographique), tandis que les autres colonnes reprÃ©sentent les annÃ©es avec les valeurs dÃ©mographiques. Cette structure est courante dans les jeux de donnÃ©es statistiques, mais doit Ãªtre nettoyÃ©e pour permettre une analyse efficace.

## Transformer les donnÃ©esÂ : diviser les champs

Passons maintenant Ã  la transformation des donnÃ©es. Nous devons nous assurer que le premier champ est divisÃ© en colonnes distinctes. De plus, nous devons Ã©galement nous assurer de travailler avec les types de donnÃ©es corrects et dâ€™appliquer des filtres. 

> **Pourquoi nous devons diviser les champs**Â : La premiÃ¨re colonne contient plusieurs informations concatÃ©nÃ©es (frÃ©quence, type de projection, sexe, tranche dâ€™Ã¢ge, unitÃ© et code gÃ©ographique). Pour une analyse correcte, chacune de ces informations doit figurer dans sa propre colonne. Ce processus est appelÃ© Â«Â normalisationÂ Â» de la structure des donnÃ©es.

1. CrÃ©ez une nouvelle cellule dans votre notebook et copiez-y lâ€™instruction suivante.


    ```copilot-prompt
    %%code
    
    From the currently loaded DataFrame, split the first field 'freq,projection,sex,age,unit,geo\TIME_PERIOD' using a comma into 6 separate fields.
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code et observer la sortie. Voici un exemple de ce Ã  quoi la sortie peut ressemblerÂ :

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import split, col
    
    # Split the first field 'freq,projection,sex,age,unit,geo\TIME_PERIOD' into 6 separate fields
    spark_df = spark_df.withColumn("freq", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(0)) \
                       .withColumn("projection", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(1)) \
                       .withColumn("sex", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(2)) \
                       .withColumn("age", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(3)) \
                       .withColumn("unit", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(4)) \
                       .withColumn("geo", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(5))
    
    # Show the updated DataFrame
    display(spark_df)
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code. Vous devrez peut-Ãªtre faire dÃ©filer la table vers la droite pour voir les nouveaux champs ajoutÃ©s Ã  la table.

    ![Capture dâ€™Ã©cran de la table obtenue avec des champs supplÃ©mentaires.](Images/copilot-fabric-notebook-split-fields.png)

## Transformer les donnÃ©esÂ : supprimer les champs

Certains champs de la table nâ€™ont aucune valeur significative, car ils ne contiennent quâ€™une seule entrÃ©e distincte. La bonne pratique consiste Ã  les supprimer du jeu de donnÃ©es.

> **Principe de nettoyage des donnÃ©es**Â : Les colonnes qui ne contiennent quâ€™une seule valeur unique nâ€™apportent aucune valeur dâ€™analyse et peuvent rendre votre jeu de donnÃ©es inutilement complexe. Les supprimer simplifie la structure des donnÃ©es et amÃ©liore les performances. Dans ce cas, Â«Â freqÂ Â» (frÃ©quence), Â«Â ageÂ Â» (tous les enregistrements indiquent TOTAL) et Â«Â unitÂ Â» (tous les enregistrements indiquent PER pour les personnes) sont constants dans toutes les lignes.

1. CrÃ©ez une nouvelle cellule dans votre notebook et copiez-y lâ€™instruction suivante.

    ```copilot-prompt
    %%code
    
    From the currently loaded DataFrame, remove the fields 'freq', 'age', 'unit'.
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code et observer la sortie. Voici un exemple de ce Ã  quoi la sortie peut ressemblerÂ :

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Remove the fields 'freq', 'age', 'unit'
    spark_df = spark_df.drop("freq", "age", "unit")
    
    # Show the updated DataFrame
    display(spark_df)
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code.

## Transformer les donnÃ©esÂ : repositionner les champs

Organiser vos donnÃ©es en plaÃ§ant les colonnes dâ€™identification les plus importantes en premier facilite la lecture et la comprÃ©hension. Dans lâ€™analyse des donnÃ©es, il est courant de placer les colonnes catÃ©gorielles/dimensionnelles ( comme le type de projection, le sexe et la situation gÃ©ographique) avant les colonnes numÃ©riques/de mesure (les valeurs de population par annÃ©e).

1. CrÃ©ez une nouvelle cellule dans votre notebook et copiez-y lâ€™instruction suivante.

    ```copilot-prompt
    %%code
    
    From the currently loaded DataFrame, the fields 'projection', 'sex', 'geo' should be positioned first.
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code et observer la sortie. Voici un exemple de ce Ã  quoi la sortie peut ressemblerÂ :

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Reorder the DataFrame with 'projection', 'sex', 'geo' fields first
    new_column_order = ['projection', 'sex', 'geo'] + [col for col in spark_df.columns if col not in {'projection', 'sex', 'geo'}]
    spark_df = spark_df.select(new_column_order)
    
    # Show the reordered DataFrame
    display(spark_df)
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code.

## Transformer les donnÃ©esÂ : remplacer les valeurs

Le champ de projection contient actuellement des codes cryptiques qui ne sont pas conviviaux. Pour amÃ©liorer la lisibilitÃ© et lâ€™analyse, nous remplacerons ces codes par des noms descriptifs qui expliquent clairement ce que reprÃ©sente chaque scÃ©nario de projection.

> **Comprendre les scÃ©narios de projection**Â : Les organismes statistiques utilisent souvent diffÃ©rents scÃ©narios pour modÃ©liser lâ€™Ã©volution future de la population. La ligne de base reprÃ©sente le scÃ©nario le plus probable, tandis que les tests de sensibilitÃ© montrent comment la population pourrait Ã©voluer selon diffÃ©rentes hypothÃ¨ses concernant les taux de fÃ©conditÃ©, de mortalitÃ© et les flux migratoires.

1. CrÃ©ez une nouvelle cellule dans votre notebook et copiez-y lâ€™instruction suivante.


    ```copilot-prompt
    %%code
    
    The 'projection' field contains codes that should be replaced with the following values:
        _'BSL' -> 'Baseline projections'.
        _'LFRT' -> 'Sensitivity test: lower fertility'.
        _'LMRT' -> 'Sensitivity test: lower mortality'.
        _'HMIGR' -> 'Sensitivity test: higher migration'.
        _'LMIGR' -> 'Sensitivity test: lower migration'.
        _'NMIGR' -> 'Sensitivity test: no migration'.
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code et observer la sortie. Voici un exemple de ce Ã  quoi la sortie peut ressemblerÂ :

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import when
    
    # Replace projection codes
    spark_df = spark_df.withColumn("projection", 
                                   when(spark_df["projection"] == "BSL", "Baseline projections")
                                   .when(spark_df["projection"] == "LFRT", "Sensitivity test: lower fertility")
                                   .when(spark_df["projection"] == "LMRT", "Sensitivity test: lower mortality")
                                   .when(spark_df["projection"] == "HMIGR", "Sensitivity test: higher migration")
                                   .when(spark_df["projection"] == "LMIGR", "Sensitivity test: lower migration")
                                   .when(spark_df["projection"] == "NMIGR", "Sensitivity test: no migration")
                                   .otherwise(spark_df["projection"]))
    
    # Display the updated DataFrame
    display(spark_df)
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code.

    ![Capture dâ€™Ã©cran de la table obtenue avec les valeurs des champs du projet remplacÃ©es.](Images/copilot-fabric-notebook-replace-values.png)
    
## Transformer les donnÃ©esÂ : filtrer les donnÃ©es

La table des projections dÃ©mographiques contient deux lignes pour des pays qui nâ€™existent pasÂ : EU27_2020 (*totaux pour lâ€™Union europÃ©enne - 27Â pays*) et EA20 (*zone euro - 20Â pays*). Nous devons supprimer ces deux lignes, car nous souhaitons ne conserver que les donnÃ©es au niveau de granularitÃ© le plus fin.

> **Principe de granularitÃ© des donnÃ©es**Â : Pour une analyse dÃ©taillÃ©e, il est important de travailler avec des donnÃ©es au niveau de granularitÃ© le plus fin possible. Les valeurs agrÃ©gÃ©es (comme les totaux de lâ€™UE) peuvent toujours Ãªtre calculÃ©es lorsque cela est nÃ©cessaire, mais leur inclusion dans votre jeu de donnÃ©es de base peut entraÃ®ner un double comptage ou une confusion dans lâ€™analyse.

![Capture dâ€™Ã©cran de la table avec geo EA20 et EU2_2020 mis en Ã©vidence.](Images/copilot-fabric-notebook-europe.png)

1. CrÃ©ez une nouvelle cellule dans votre notebook et copiez-y lâ€™instruction suivante.

    ```copilot-prompt
    %%code
    
    Filter the 'geo' field and remove values 'EA20' and 'EU27_2020' (these are not countries).
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code et observer la sortie. Voici un exemple de ce Ã  quoi la sortie peut ressemblerÂ :

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Filter out 'geo' values 'EA20' and 'EU27_2020'
    spark_df = spark_df.filter((spark_df['geo'] != 'EA20') & (spark_df['geo'] != 'EU27_2020'))
    
    # Display the filtered DataFrame
    display(spark_df)
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code.

    La table du projet dÃ©mographique contient Ã©galement un champ Â«Â sexeÂ Â» qui contient les valeurs distinctes suivantesÂ :
    
    - MÂ : Male
    - F: Femme
    - T: Total (homme + femme)

    Une fois encore, nous devons supprimer les totaux afin de conserver les donnÃ©es au niveau de dÃ©tail le plus bas possible.

    > **Pourquoi supprimer les totaux**Â : Comme pour les agrÃ©gations gÃ©ographiques, nous souhaitons conserver uniquement les catÃ©gories de sexe individuelles (Homme et Femme) et exclure les valeurs totales. Cela permet une analyse plus flexibleÂ : vous pouvez toujours additionner les valeurs Homme et Femme pour obtenir des totaux, mais vous ne pouvez pas diviser les totaux en composants.

1. CrÃ©ez une nouvelle cellule dans votre notebook et copiez-y lâ€™instruction suivante.

    ```copilot-prompt
    %%code
    
    Filter the 'sex' field and remove 'T' (these are totals).
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code et observer la sortie. Voici un exemple de ce Ã  quoi la sortie peut ressemblerÂ :

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Filter out 'sex' values 'T'
    spark_df = spark_df.filter(spark_df['sex'] != 'T')
    
    # Display the filtered DataFrame
    display(spark_df)
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code.

## Transformer les donnÃ©esÂ : dÃ©couper des espaces

Certains noms de champs dans la table des projections dÃ©mographiques comportent un espace Ã  la fin. Nous devons appliquer une opÃ©ration de dÃ©coupage aux noms de ces champs.

> **ProblÃ¨me liÃ© Ã  la qualitÃ© des donnÃ©es**Â : Les espaces supplÃ©mentaires dans les noms de colonnes peuvent poser des problÃ¨mes lors de la requÃªte de donnÃ©es ou de la crÃ©ation de visualisations. Il sâ€™agit dâ€™un problÃ¨me courant de qualitÃ© des donnÃ©es, en particulier lorsque les donnÃ©es proviennent de sources externes ou sont exportÃ©es Ã  partir dâ€™autres systÃ¨mes. Le dÃ©coupage des espaces garantit la cohÃ©rence et Ã©vite les problÃ¨mes difficiles Ã  dÃ©boguer par la suite.

1. CrÃ©ez une nouvelle cellule dans votre notebook et copiez-y lâ€™instruction suivante.

    ```copilot-prompt
    %%code
    
    Strip spaces from all field names in the dataframe.
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code et observer la sortie. Voici un exemple de ce Ã  quoi la sortie peut ressemblerÂ :

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import col
    
    # Strip spaces from all field names
    spark_df = spark_df.select([col(column).alias(column.strip()) for column in spark_df.columns])
    
    # Display the updated DataFrame
    display(spark_df)
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code.

## Transformer les donnÃ©esÂ : conversion des types de donnÃ©es

Si nous voulons analyser correctement les donnÃ©es ultÃ©rieurement (Ã  lâ€™aide de PowerÂ BI ou SQL par exemple), nous devons nous assurer que les types de donnÃ©es (tels que les nombres et les dates/heures) sont correctement dÃ©finis. 

> **Importance des types de donnÃ©es corrects**Â : Lorsque les donnÃ©es sont chargÃ©es Ã  partir de fichiers texte, toutes les colonnes sont initialement traitÃ©es comme des chaÃ®nes. La conversion des colonnes dâ€™annÃ©e en nombres entiers permet dâ€™effectuer des opÃ©rations mathÃ©matiques ( comme des calculs et des agrÃ©gations) et garantit un tri correct. Cette Ã©tape est cruciale pour les outils dâ€™analyse et de visualisation en aval.

1. CrÃ©ez une nouvelle cellule dans votre notebook et copiez-y lâ€™instruction suivante.

    ```copilot-prompt
    %%code
    
    Convert the data type of all the year fields to integer.
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code et observer la sortie. Voici un exemple de ce Ã  quoi la sortie peut ressemblerÂ :

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import col
    
    # Convert the data type of all the year fields to integer
    year_columns = [col(column).cast("int") for column in spark_df.columns if column.strip().isdigit()]
    spark_df = spark_df.select(*spark_df.columns[:3], *year_columns)
    
    # Display the updated DataFrame
    display(spark_df)
    ```
    
1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code. Voici un exemple de ce Ã  quoi pourrait ressembler la sortie (colonnes et lignes supprimÃ©es pour plus de concision)Â :

|          projection|sexe|geo|    2022|    2023|     ...|    2100|
|--------------------|---|---|--------|--------|--------|--------| 
|Protection de la ligne de base|  F| AT| 4553444| 4619179|     ...| 4807661|
|Protection de la ligne de base|  F| BE| 5883978| 5947528|     ...| 6331785|
|Protection de la ligne de base|  F| BG| 3527626| 3605059|     ...| 2543673|
|...                 |...|...|     ...|     ...|     ...|     ...|
|Protection de la ligne de base|  F| LU|  320333|  329401|     ...|  498954|

>[!TIP]
> Vous devrez peut-Ãªtre faire dÃ©filer la table vers la droite pour voir toutes les colonnes.

## Enregistrer des donnÃ©es

Ensuite, nous voulons enregistrer les donnÃ©es transformÃ©es dans notre lakehouse. 

> **Pourquoi enregistrer les donnÃ©es transformÃ©es**Â : AprÃ¨s tout ce travail de nettoyage et de transformation des donnÃ©es, nous souhaitons conserver les rÃ©sultats. Enregistrer les donnÃ©es sous forme de table dans le lakehouse nous permet, ainsi quâ€™Ã  dâ€™autres, dâ€™utiliser ce jeu de donnÃ©es propre pour divers scÃ©narios dâ€™analyse sans avoir Ã  rÃ©pÃ©ter le processus de transformation. Cela permet Ã©galement Ã  dâ€™autres outils de lâ€™Ã©cosystÃ¨me Microsoft Fabric (comme PowerÂ BI, SQL Analytics Endpoint et Data Factory) dâ€™utiliser ces donnÃ©es.

1. CrÃ©ez une nouvelle cellule dans votre notebook et copiez-y lâ€™instruction suivante.

    ```copilot-prompt
    %%code
    
    Save the dataframe as a new table named 'Population' in the default lakehouse.
    ```
    
1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code. Copilot gÃ©nÃ¨re le code, qui peut varier lÃ©gÃ¨rement en fonction de votre environnement et des derniÃ¨res mises Ã  jour de Copilot.

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    spark_df.write.format("delta").saveAsTable("Population")
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code.

## ValidationÂ : poser des questions

Explorons maintenant la puissance de Copilot pour lâ€™analyse des donnÃ©es. Au lieu dâ€™Ã©crire des requÃªtes SQL complexes ou du code de visualisation Ã  partir de zÃ©ro, nous pouvons poser Ã  Copilot des questions en langage naturel sur nos donnÃ©es et il gÃ©nÃ©rera le code appropriÃ© pour y rÃ©pondre.

1. Pour vÃ©rifier que les donnÃ©es sont correctement enregistrÃ©es, dÃ©veloppez les tables dans votre Lakehouse et vÃ©rifiez leur contenu (vous devrez peut-Ãªtre actualiser le dossier Tables en sÃ©lectionnant les trois points ...). 

    ![Capture dâ€™Ã©cran de lakehouse contenant dÃ©sormais une nouvelle table nommÃ©e Â«Â PopulationÂ Â».](Images/copilot-fabric-notebook-step-5-lakehouse-refreshed.png)

1. Dans le ruban Accueil, sÃ©lectionnez lâ€™option Copilot.

    > **Interface de conversation Copilot**Â : Le panneau Copilot fournit une interface conversationnelle grÃ¢ce Ã  laquelle vous pouvez poser des questions sur vos donnÃ©es en langage naturel. Il peut gÃ©nÃ©rer du code pour lâ€™analyse, crÃ©er des visualisations et vous aider Ã  explorer les modÃ¨les dans votre jeu de donnÃ©es.

    ![Capture dâ€™Ã©cran dâ€™un notebook avec le panneau Copilot ouvert.](Images/copilot-fabric-notebook-step-6-copilot-pane.png)

1. Entrez lâ€™invite suivante :

    ```copilot-prompt
    What are the projected population trends for geo BE  from 2020 to 2050 as a line chart visualization. Make sure to sum up male and female numbers. Use only existing columns from the population table. Perform the query using SQL.
    ```

    > **Ce que cela dÃ©montre**Â : Ce prompt met en Ã©vidence la capacitÃ© de Copilot Ã  comprendre le contexte (notre table Population), Ã  gÃ©nÃ©rer des requÃªtes SQL et Ã  crÃ©er des visualisations. Il est particuliÃ¨rement puissant car il combine la requÃªte de donnÃ©es et la visualisation en une seule requÃªte.

1. Observez la sortie gÃ©nÃ©rÃ©e, qui peut varier lÃ©gÃ¨rement en fonction de votre environnement et des derniÃ¨res mises Ã  jour de Copilot. Copiez le fragment de code dans une nouvelle cellule.

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    import plotly.graph_objs as go
    
    # Perform the SQL query to get projected population trends for geo BE, summing up male and female numbers
    result = spark.sql(
        """
        SELECT projection, sex, geo, SUM(`2022`) as `2022`, SUM(`2023`) as `2023`, SUM(`2025`) as `2025`,
               SUM(`2030`) as `2030`, SUM(`2035`) as `2035`, SUM(`2040`) as `2040`,
               SUM(`2045`) as `2045`, SUM(`2050`) as `2050`
        FROM Population
        WHERE geo = 'BE' AND projection = 'Baseline projections'
        GROUP BY projection, sex, geo
        """
    )
    df = result.groupBy("projection").sum()
    df = df.orderBy("projection").toPandas()
    
    # Extract data for the line chart
    years = df.columns[1:].tolist()
    values = df.iloc[0, 1:].tolist()
    
    # Create the plot
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=years, y=values, mode='lines+markers', name='Projected Population'))
    
    # Update the layout
    fig.update_layout(
        title='Projected Population Trends for Geo BE (Belgium) from 2022 to 2050',
        xaxis_title='Year',
        yaxis_title='Population',
        template='plotly_dark'
    )
    
    # Display the plot
    fig.show()
    ```

1. SÃ©lectionnez â–· **ExÃ©cuter la cellule** Ã  gauche de la cellule pour exÃ©cuter le code. 

    Observez le graphique qui a Ã©tÃ© crÃ©Ã©Â :
    
    ![Capture dâ€™Ã©cran du notebook avec le graphique en courbes crÃ©Ã©.](Images/copilot-fabric-notebook-step-7-line-chart.png)
    
    > **Ce que vous avez accompli**Â : Vous avez utilisÃ© Copilot avec succÃ¨s pour gÃ©nÃ©rer une visualisation qui montre les tendances dÃ©mographiques en Belgique au fil du temps. Cela illustre le flux de travail complet dâ€™ingÃ©nierie des donnÃ©esÂ : ingestion des donnÃ©es, transformation, stockage et analyse, le tout avec lâ€™aide de lâ€™IA.

## Nettoyer les ressources

Dans cet exercice, vous avez appris Ã  utiliser Copilot et Spark pour travailler avec des donnÃ©es dans Microsoft Fabric.

Si vous avez terminÃ© dâ€™explorer vos donnÃ©es, vous pouvez mettre fin Ã  la session Spark et supprimer lâ€™espace de travail que vous avez crÃ©Ã© pour cet exercice.

1.  Dans le menu du notebook, sÃ©lectionnez **ArrÃªter la session** pour mettre fin Ã  la session Spark.
1.  Dans la barre de gauche, sÃ©lectionnez lâ€™icÃ´ne de votre espace de travail pour afficher tous les Ã©lÃ©ments quâ€™il contient.
1.  SÃ©lectionnez **ParamÃ¨tres de lâ€™espace de travail** et, dans la section**GÃ©nÃ©ral**, faites dÃ©filer vers le bas et sÃ©lectionnez **Supprimer cet espace de travail**.
1.  SÃ©lectionnez **Supprimer** pour supprimer lâ€™espace de travail.
