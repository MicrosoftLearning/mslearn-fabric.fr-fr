---
lab:
  title: Utiliser des tables delta dans Apache Spark
  module: Work with Delta Lake tables in Microsoft Fabric
---

# Utiliser des tables delta dans Apache Spark

Les tables d‚Äôun lakehouse Microsoft¬†Fabric sont bas√©es sur le format de fichier open¬†source Delta¬†Lake. Delta¬†Lake ajoute la prise en charge de la s√©mantique relationnelle pour les donn√©es de diffusion et de lots. Dans cet exercice, vous allez cr√©er des tables Delta et explorer les donn√©es √† l‚Äôaide de requ√™tes SQL.

Cet exercice devrait prendre environ **45**¬†minutes

> [!NOTE]
> Vous devez disposer d‚Äôune version d‚Äô√©valuation [Microsoft¬†Fabric](/fabric/get-started/fabric-trial) pour effectuer cet exercice.

## Cr√©er un espace de travail

Tout d‚Äôabord, cr√©ez un espace de travail avec la *version d‚Äô√©valuation de Fabric* activ√©e.

1. Acc√©dez √† la [page d‚Äôaccueil de Microsoft¬†Fabric](https://app.fabric.microsoft.com/home?experience=fabric) sur `https://app.fabric.microsoft.com/home?experience=fabric` dans un navigateur et connectez-vous avec vos informations d‚Äôidentification Fabric.
1. Dans la barre de menus √† gauche, s√©lectionnez **Espaces de travail** (üóá).
1. Cr√©ez un **nouvel espace de travail** avec le nom de votre choix et s√©lectionnez un mode de licence qui inclut la capacit√© Fabric (Essai, Premium ou Fabric).
1. Lorsque votre nouvel espace de travail s‚Äôouvre, il doit √™tre vide.

    ![Image d‚Äô√©cran d‚Äôun espace de travail Fabric vide.](Images/workspace-empty.jpg)

## Cr√©er un lakehouse et charger des donn√©es

Maintenant que vous disposez d‚Äôun espace de travail, il est temps de cr√©er un lakehouse et de charger vos donn√©es.

1. S√©lectionnez **Cr√©er** dans la barre de menus de gauche. Dans la page *Nouveau*, sous la section *Engineering donn√©es*, s√©lectionnez **Lakehouse**. Donnez-lui un nom unique de votre choix.

    >**Note**¬†: si l‚Äôoption **Cr√©er** n‚Äôest pas √©pingl√©e √† la barre lat√©rale, vous devez d‚Äôabord s√©lectionner l‚Äôoption avec des points de suspension (**...**).

1. Il existe plusieurs fa√ßons d‚Äôing√©rer les donn√©es, mais dans cet exercice, vous allez t√©l√©charger un fichier texte sur votre ordinateur local (ou sur votre machine virtuelle de labo le cas √©ch√©ant), puis le charger dans votre lakehouse. T√©l√©chargez le [fichier de donn√©es](https://github.com/MicrosoftLearning/dp-data/raw/main/products.csv) √† partir de `https://github.com/MicrosoftLearning/dp-data/raw/main/products.csv`, en l‚Äôenregistrant en tant que *products.csv*.
1.  Revenez √† l‚Äôonglet du navigateur web contenant votre lakehouse, puis, dans le volet Explorateur, en regard du dossier **Fichiers**, s√©lectionnez le menu ‚Ä¶ .  Cr√©ez un **sous-dossier** appel√© *produits*.
1.  Dans le menu ‚Ä¶ du dossier produits, **chargez** le fichier *products.csv* √† partir de votre ordinateur local (ou de votre machine virtuelle de labo le cas √©ch√©ant).
1.  Une fois le fichier charg√©, s√©lectionnez le dossier **products** et v√©rifiez que le fichier a √©t√© charg√©, comme montr√© ici¬†:

    ![Image d‚Äô√©cran de products.csv charg√© dans le lakehouse.](Images/upload-products.jpg)
  
## Explorer les donn√©es dans un DataFrame

1.  Cr√©ez un **notebook**. Apr√®s quelques secondes, un nouveau notebook contenant une seule cellule s‚Äôouvre. Les notebooks sont constitu√©s d‚Äôune ou plusieurs cellules qui peuvent contenir du code ou du Markdown (texte mis en forme).
2.  S√©lectionnez la premi√®re cellule (qui est actuellement une cellule de code) puis, dans la barre d‚Äôoutils en haut √† droite, utilisez le bouton **M‚Üì** pour convertir la cellule en cellule Markdown. Le texte contenu dans la cellule s‚Äôaffiche alors sous forme de texte mis en forme. Utilisez des cellules Markdown pour fournir des √©l√©ments d‚Äôexplication sur votre code.
3.  Utilisez le bouton üñâ (Modifier) pour placer la cellule en mode √©dition, puis modifiez le balisage Markdown comme suit¬†:

    ```markdown
    # Delta Lake tables 
    Use this notebook to explore Delta Lake functionality 
    ```

4. Cliquez n‚Äôimporte o√π dans le notebook en dehors de la cellule pour arr√™ter sa modification et voir le balisage Markdown rendu.
5. Ajoutez une nouvelle cellule de code et ajoutez le code suivant pour lire les donn√©es des produits dans un DataFrame √† l‚Äôaide d‚Äôun sch√©ma d√©fini¬†:

    ```python
    from pyspark.sql.types import StructType, IntegerType, StringType, DoubleType

    # define the schema
    schema = StructType() \
    .add("ProductID", IntegerType(), True) \
    .add("ProductName", StringType(), True) \
    .add("Category", StringType(), True) \
    .add("ListPrice", DoubleType(), True)

    df = spark.read.format("csv").option("header","true").schema(schema).load("Files/products/products.csv")
    # df now is a Spark DataFrame containing CSV data from "Files/products/products.csv".
    display(df)
    ```

> [!TIP]
> Masquez ou affichez les volets de l‚ÄôExplorateur √† l‚Äôaide de l‚Äôic√¥ne de chevron ¬´. Cela vous permet de vous concentrer sur le notebook ou vos fichiers.

7. Utilisez le bouton **Ex√©cuter la cellule** (‚ñ∑) √† gauche de la cellule pour l‚Äôex√©cuter.

> [!NOTE]
> Comme c‚Äôest la premi√®re fois que vous ex√©cutez du code dans ce notebook, une session Spark doit √™tre d√©marr√©e. Cela signifie que la premi√®re ex√©cution peut prendre environ une minute. Les ex√©cutions suivantes seront plus rapides.

8. Une fois le code de la cellule ex√©cut√©, examinez la sortie sous la cellule, qui doit √™tre similaire √† ceci¬†:

    ![Image d‚Äô√©cran des donn√©es products.csv.](Images/products-schema.jpg)
 
## Cr√©er des tables delta

Vous pouvez enregistrer le DataFrame en tant que table delta en utilisant la m√©thode *saveAsTable*. Delta Lake prend en charge la cr√©ation de tables manag√©es et externes.

   * Les tables delta **manag√©es** b√©n√©ficient de performances plus √©lev√©es, car Fabric g√®re les m√©tadonn√©es de sch√©ma et les fichiers de donn√©es.
   * Les tables **externes** vous permettent de stocker des donn√©es en externe, avec les m√©tadonn√©es g√©r√©es par Fabric.

### Cr√©er une table manag√©e

Les fichiers de donn√©es sont cr√©√©s dans le dossier **Tables**.

1. Sous les r√©sultats retourn√©s par la premi√®re cellule de code, utilisez l‚Äôic√¥ne +Code pour ajouter une nouvelle cellule de code.

> [!TIP]
> Pour afficher l‚Äôic√¥ne +Code, d√©placez la souris juste en dessous et √† gauche de la sortie de la cellule active. Sinon, dans la barre de menus, dans l‚Äôonglet Modifier, s√©lectionnez **+Ajouter une cellule de code**.

2. Pour cr√©er une table delta manag√©e, ajoutez une nouvelle cellule, entrez le code suivant, puis ex√©cutez la cellule¬†:

    ```python
    df.write.format("delta").saveAsTable("managed_products")
    ```

3.  Dans le volet Explorateur Lakehouse, **actualisez** le dossier Tables et d√©veloppez le n≈ìud Tables pour v√©rifier que la table **managed_products** a √©t√© cr√©√©e.

>[!NOTE]
> L‚Äôic√¥ne de triangle en regard du nom de fichier indique une table delta.

Les fichiers des tables manag√©es sont stock√©s dans le dossier **Tables** dans le lakehouse. Un dossier nomm√© *managed_products* a √©t√© cr√©√© pour stocker les fichiers Parquet, et le dossier delta_log de la table.

### Cr√©er une table externe

Vous pouvez √©galement cr√©er des tables externes, qui peuvent √™tre stock√©es ailleurs que dans le lakehouse, avec les m√©tadonn√©es de sch√©ma stock√©es dans le lakehouse.

1.  Dans le volet Explorateur Lakehouse, dans le menu ‚Ä¶ du dossier **Fichiers**, s√©lectionnez **Copier le chemin ABFS**. Le chemin ABFS est le chemin d‚Äôacc√®s complet du dossier Files du lakehouse.

2.  Dans une nouvelle cellule de code, collez le chemin ABFS. Ajoutez le code suivant, √† l‚Äôaide de couper-coller pour ins√©rer le abfs_path au bon endroit dans le code¬†:

    ```python
    df.write.format("delta").saveAsTable("external_products", path="abfs_path/external_products")
    ```

3. Le chemin complet doit √™tre similaire a ceci¬†:

    ```python
    abfss://workspace@tenant-onelake.dfs.fabric.microsoft.com/lakehousename.Lakehouse/Files/external_products
    ```

4. **Ex√©cutez** la cellule pour enregistrer le DataFrame en tant que table externe dans le dossier Files/external_products.

5.  Dans le volet Explorateur Lakehouse, **actualisez** le dossier Tables, d√©veloppez le n≈ìud Tables et v√©rifiez que la table external_products a √©t√© cr√©√©e et qu‚Äôelle contient les m√©tadonn√©es du sch√©ma.

6.  Dans le volet Explorateur Lakehouse, dans le menu ‚Ä¶ du dossier Files, s√©lectionnez **Actualiser**. D√©veloppez ensuite le n≈ìud Files et v√©rifiez que le dossier external_products a √©t√© cr√©√© pour les fichiers de donn√©es de la table.

### Comparer les tables manag√©es et les tables externes

Explorons les diff√©rences entre les tables manag√©es et les tables externes √† l‚Äôaide de la commande magic %%sql.

1. Dans une nouvelle cellule de code, ex√©cutez le code suivant¬†:

    ```python
    %%sql
    DESCRIBE FORMATTED managed_products;
    ```

2. Dans les r√©sultats, affichez la propri√©t√© Emplacement de la table. Cliquez sur la valeur Emplacement dans la colonne Type de donn√©es pour afficher le chemin d‚Äôacc√®s complet. Notez que l‚Äôemplacement de stockage OneLake se termine par /Tables/managed_products.

3. Modifiez la commande DESCRIBE pour afficher les d√©tails de la table external_products comme indiqu√© ici¬†:

    ```python
    %%sql
    DESCRIBE FORMATTED external_products;
    ```

4. Ex√©cutez la cellule et, dans les r√©sultats, affichez la propri√©t√© Emplacement de la table. √âtendez la colonne Type de donn√©es pour voir le chemin d‚Äôacc√®s complet et notez que les emplacements de stockage OneLake se terminent par /Files/external_products.

5. Dans une nouvelle cellule de code, ex√©cutez le code suivant¬†:

    ```python
    %%sql
    DROP TABLE managed_products;
    DROP TABLE external_products;
    ```

6. Dans le volet Explorateur Lakehouse, **actualisez** le dossier Tables pour v√©rifier qu‚Äôaucune table n‚Äôest r√©pertori√©e dans le n≈ìud Tables.
7.  Dans le volet Explorateur Lakehouse, **actualisez** le dossier Files et v√©rifiez que le fichier external_products n‚Äôa *pas* √©t√© supprim√©. S√©lectionnez ce dossier pour afficher les fichiers de donn√©es Parquet et le dossier _delta_log. 

Les m√©tadonn√©es de table pour la table externe ont √©t√© supprim√©es, mais pas les fichiers de donn√©es.

## Utiliser SQL pour cr√©er une table delta

Vous allez maintenant cr√©er une table delta √† l‚Äôaide de la commande magic %%sql. 

1. Ajoutez une autre cellule de code et ex√©cutez le code suivant¬†:

    ```python
    %%sql
    CREATE TABLE products
    USING DELTA
    LOCATION 'Files/external_products';
    ```

2. Dans le volet Explorateur Lakehouse, dans le menu ‚Ä¶ du dossier **Tables**, s√©lectionnez **Actualiser**. D√©veloppez ensuite le n≈ìud Tables et v√©rifiez qu‚Äôune nouvelle table nomm√©e *products* est list√©e. D√©veloppez ensuite la table pour afficher le sch√©ma.

3. Ajoutez une autre cellule de code et ex√©cutez le code suivant¬†:

    ```python
    %%sql
    SELECT * FROM products;
    ```

## Explorer le versioning des tables

L‚Äôhistorique des transactions des tables delta est stock√© dans des fichiers JSON, dans le dossier delta_log. Vous pouvez utiliser ce journal des transactions pour g√©rer le versioning des donn√©es.

1.  Ajoutez une nouvelle cellule de code au notebook et ex√©cutez le code suivant qui impl√©mente une r√©duction de 10¬†% du prix des VTT¬†:

    ```python
    %%sql
    UPDATE products
    SET ListPrice = ListPrice * 0.9
    WHERE Category = 'Mountain Bikes';
    ```

2. Ajoutez une autre cellule de code et ex√©cutez le code suivant¬†:

    ```python
    %%sql
    DESCRIBE HISTORY products;
    ```

Les r√©sultats montrent l‚Äôhistorique des transactions enregistr√©es pour la table.

3.  Ajoutez une autre cellule de code et ex√©cutez le code suivant¬†:

    ```python
    delta_table_path = 'Files/external_products'
    # Get the current data
    current_data = spark.read.format("delta").load(delta_table_path)
    display(current_data)

    # Get the version 0 data
    original_data = spark.read.format("delta").option("versionAsOf", 0).load(delta_table_path)
    display(original_data)
    ```

Deux jeux de r√©sultats son renvoy√©s¬†: un contenant les donn√©es apr√®s la r√©duction du prix et l‚Äôautre montrant la version d‚Äôorigine des donn√©es.

## Analyser les donn√©es de table Delta avec des requ√™tes SQL

√Ä l‚Äôaide de la commande magic SQL, vous pouvez utiliser la syntaxe SQL au lieu de Pyspark. Ici, vous allez cr√©er une vue temporaire √† partir de la table products √† l‚Äôaide d‚Äôune instruction `SELECT`.

1. Ajoutez une nouvelle cellule de code, puis ex√©cutez le code suivant pour cr√©er et afficher l‚Äôaffichage temporaire¬†:

    ```python
    %%sql
    -- Create a temporary view
    CREATE OR REPLACE TEMPORARY VIEW products_view
    AS
        SELECT Category, COUNT(*) AS NumProducts, MIN(ListPrice) AS MinPrice, MAX(ListPrice) AS MaxPrice, AVG(ListPrice) AS AvgPrice
        FROM products
        GROUP BY Category;

    SELECT *
    FROM products_view
    ORDER BY Category;    
    ```

2. Ajoutez une nouvelle cellule de code et ex√©cutez le code suivant pour retourner les 10¬†premi√®res cat√©gories par nombre de produits¬†:

    ```python
    %%sql
    SELECT Category, NumProducts
    FROM products_view
    ORDER BY NumProducts DESC
    LIMIT 10;
    ```

3. Lorsque les donn√©es sont retourn√©es, s√©lectionnez la vue **Graphique** pour afficher un graphique √† barres.

    ![Image d‚Äô√©cran des r√©sultats et de l‚Äôinstruction select SQL.](Images/sql-select.jpg)

Vous pouvez √©galement ex√©cuter une requ√™te SQL √† l‚Äôaide de PySpark.

4. Ajoutez une nouvelle cellule de code, puis ex√©cutez le code suivant¬†:

    ```python
    from pyspark.sql.functions import col, desc

    df_products = spark.sql("SELECT Category, MinPrice, MaxPrice, AvgPrice FROM products_view").orderBy(col("AvgPrice").desc())
    display(df_products.limit(6))
    ```

## Utiliser des tables delta pour les donn√©es de diffusion

Delta Lake prend en charge les donn√©es de diffusion. Les tables delta peuvent √™tre un r√©cepteur ou une source pour des flux de donn√©es cr√©√©s en utilisant l‚ÄôAPI Spark Structured Streaming. Dans cet exemple, vous allez utiliser une table delta comme r√©cepteur pour des donn√©es de diffusion dans un sc√©nario Internet des objets (IoT) simul√©.

1.  Ajoutez une nouvelle cellule de code, ajoutez le code suivant et ex√©cutez-le¬†:

    ```python
    from notebookutils import mssparkutils
    from pyspark.sql.types import *
    from pyspark.sql.functions import *

    # Create a folder
    inputPath = 'Files/data/'
    mssparkutils.fs.mkdirs(inputPath)

    # Create a stream that reads data from the folder, using a JSON schema
    jsonSchema = StructType([
    StructField("device", StringType(), False),
    StructField("status", StringType(), False)
    ])
    iotstream = spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger", 1).json(inputPath)

    # Write some event data to the folder
    device_data = '''{"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"error"}
    {"device":"Dev2","status":"ok"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}'''

    mssparkutils.fs.put(inputPath + "data.txt", device_data, True)

    print("Source stream created...")
    ```

V√©rifiez que le message *Flux source cr√©√©‚Ä¶* est affich√©e. Le code que vous venez d‚Äôex√©cuter a cr√©√© une source de donn√©es de streaming bas√©e sur un dossier dans lequel des donn√©es ont √©t√© enregistr√©es, repr√©sentant les lectures d‚Äôappareils IoT hypoth√©tiques.

2. Dans une nouvelle cellule de code, ajoutez et ex√©cutez le code suivant¬†:

    ```python
    # Write the stream to a delta table
    delta_stream_table_path = 'Tables/iotdevicedata'
    checkpointpath = 'Files/delta/checkpoint'
    deltastream = iotstream.writeStream.format("delta").option("checkpointLocation", checkpointpath).start(delta_stream_table_path)
    print("Streaming to delta sink...")
    ```

Ce code √©crit les donn√©es des appareils de diffusion en continu au format delta dans un dossier nomm√© iotdevicedata. En raison du chemin de l‚Äôemplacement du dossier dans le dossier Tables, une table sera cr√©√©e automatiquement pour celui-ci.

3. Dans une nouvelle cellule de code, ajoutez et ex√©cutez le code suivant¬†:

    ```python
    %%sql
    SELECT * FROM IotDeviceData;
    ```

Ce code interroge la table IotDeviceData, qui contient les donn√©es des appareils provenant de la source de streaming.

4. Dans une nouvelle cellule de code, ajoutez et ex√©cutez le code suivant¬†:

    ```python
    # Add more data to the source stream
    more_data = '''{"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"error"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}'''

    mssparkutils.fs.put(inputPath + "more-data.txt", more_data, True)
    ```

Ce code √©crit plus de donn√©es d‚Äôappareils hypoth√©tiques dans la source de streaming.

5. R√©ex√©cutez la cellule contenant le code suivant¬†:

    ```python
    %%sql
    SELECT * FROM IotDeviceData;
    ```

Ce code interroge √† nouveau la table IotDeviceData, qui doit maintenant inclure les donn√©es suppl√©mentaires qui ont √©t√© ajout√©es √† la source de streaming.

6. Dans une nouvelle cellule de code, ajoutez du code pour arr√™ter le flux et ex√©cuter la cellule¬†:

    ```python
    deltastream.stop()
    ```

## Nettoyer les ressources

Dans cet exercice, vous avez d√©couvert comment travailler avec des tables delta dans Microsoft¬†Fabric.

Si vous avez termin√© d‚Äôexplorer votre lakehouse, vous pouvez supprimer l‚Äôespace de travail que vous avez cr√©√© pour cet exercice.

1. Dans la barre de gauche, s√©lectionnez l‚Äôic√¥ne de votre espace de travail pour afficher tous les √©l√©ments qu‚Äôil contient.
2. Dans le menu ‚Ä¶ de la barre d‚Äôoutils, s√©lectionnez **Param√®tres des espaces de travail**.
3. Dans la section G√©n√©ral, s√©lectionnez **Supprimer cet espace de travail**.
